{
  "cells": [
    {
    "cell_type": "markdown",
    "metadata": {},
    "source": [
      "# 02 - Preprocessing Dataset Industrial IoT\n",
      "\n",
      "Questo notebook preprocessa il dataset dei dispositivi IoT industriali:\n",
      "- Pulizia dei dati e gestione valori mancanti\n",
      "- Feature engineering e creazione nuove variabili\n",
      "- Normalizzazione e standardizzazione\n",
      "- Gestione outliers\n",
      "- Divisione train/test set\n",
      "- Salvataggio dati processati"
    ]
    },

    {
    "cell_type": "code",
    "execution_count": null,
    "metadata": {},
    "outputs": [],
    "source": [
      "# Import delle librerie necessarie\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
      "from sklearn.impute import SimpleImputer\n",
      "import yaml\n",
      "import joblib\n",
      "import warnings\n",
      "import os\n",
      "\n",
      "warnings.filterwarnings('ignore')\n",
      "plt.style.use('seaborn-v0_8')\n",
      "sns.set_palette(\"husl\")\n",
      "\n",
      "# Configurazione per i grafici\n",
      "plt.rcParams['figure.figsize'] = (12, 8)\n",
      "plt.rcParams['font.size'] = 12"
    ]
    },

    {
    "cell_type": "code",
    "execution_count": null,
    "metadata": {},
    "outputs": [],
    "source": [
      "# Caricamento configurazione e dataset\n",
      "with open('../config.yaml', 'r') as file:\n",
      "    config = yaml.safe_load(file)\n",
      "\n",
      "# Caricamento dataset originale\n",
      "data_path = '../' + config['data']['raw_data_path']\n",
      "df = pd.read_csv(data_path)\n",
      "\n",
      "print(f\"Dataset caricato: {df.shape}\")\n",
      "print(f\"Memoria utilizzata: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")"
    ]
    },

    {
    "cell_type": "markdown",
    "metadata": {},
    "source": [
      "## 1. Pulizia Iniziale dei Dati"
    ]
    },

    {
    "cell_type": "code",
    "execution_count": null,
    "metadata": {},
    "outputs": [],
    "source": [
      "# Controllo duplicati\n",
      "print(\"=== CONTROLLO DUPLICATI ===\")\n",
      "duplicates = df.duplicated().sum()\n",
      "print(f\"Righe duplicate: {duplicates}\")\n",
      "\n",
      "if duplicates > 0:\n",
      "    print(\"Rimozione duplicati...\")\n",
      "    df = df.drop_duplicates()\n",
      "    print(f\"Nuova forma dataset: {df.shape}\")\n",
      "\n",
      "# Reset dell'indice\n",
      "df = df.reset_index(drop=True)\n",
      "\n",
      "# Controllo valori mancanti per colonna\n",
      "print(\"\\n=== VALORI MANCANTI ===\")\n",
      "missing_data = df.isnull().sum()\n",
      "missing_percent = (missing_data / len(df)) * 100\n",
      "\n",
      "missing_df = pd.DataFrame({\n",
      "    'Colonna': missing_data.index,\n",
      "    'Valori_Mancanti': missing_data.values,\n",
      "    'Percentuale': missing_percent.values\n",
      "})\n",
      "\n",
      "missing_df = missing_df[missing_df['Valori_Mancanti'] > 0].sort_values('Percentuale', ascending=False)\n",
      "if len(missing_df) > 0:\n",
      "    display(missing_df)\n",
      "else:\n",
      "    print(\"‚úÖ Nessun valore mancante trovato!\")"
    ]
    },

    {
    "cell_type": "code",
    "execution_count": null,
    "metadata": {},
    "outputs": [],
    "source": [
      "# Classificazione dispositivi secondo le specifiche del progetto\n",
      "devices_common_only = config['devices_common_only']\n",
      "laser_devices = config['devices_with_additional']['laser_devices']\n",
      "hydraulic_devices = config['devices_with_additional']['hydraulic_devices']\n",
      "coolant_devices = config['devices_with_additional']['coolant_devices']\n",
      "heat_devices = config['devices_with_additional']['heat_devices']\n",
      "\n",
      "def classify_device(machine_type):\n",
      "    if machine_type in devices_common_only:\n",
      "        return 'Solo Comuni'\n",
      "    elif machine_type in laser_devices:\n",
      "        return 'Laser'\n",
      "    elif machine_type in hydraulic_devices:\n",
      "        return 'Idraulico'\n",
      "    elif machine_type in coolant_devices:\n",
      "        return 'Refrigerante'\n",
      "    elif machine_type in heat_devices:\n",
      "        return 'Calore'\n",
      "    else:\n",
      "        return 'Altro'\n",
      "\n",
      "df['Device_Category'] = df['Machine_Type'].apply(classify_device)\n",
      "print(\"‚úÖ Classificazione dispositivi completata\")"
    ]
    },

    {
    "cell_type": "markdown",
    "metadata": {},
    "source": [
      "## 2. Gestione Valori Mancanti"
    ]
    },

    {
    "cell_type": "code",
    "execution_count": null,
    "metadata": {},
    "outputs": [],
    "source": [
      "# Gestione valori mancanti per features aggiuntive\n",
      "print(\"=== GESTIONE VALORI MANCANTI FEATURES AGGIUNTIVE ===\")\n",
      "\n",
      "# Le features aggiuntive devono essere nulle per i dispositivi che non le utilizzano\n",
      "additional_features = config['features']['additional_features']\n",
      "\n",
      "# Laser_Intensity: solo per dispositivi laser\n",
      "df.loc[~df['Machine_Type'].isin(laser_devices), 'Laser_Intensity'] = np.nan\n",
      "\n",
      "# Hydraulic_Pressure_bar: solo per dispositivi idraulici\n",
      "df.loc[~df['Machine_Type'].isin(hydraulic_devices), 'Hydraulic_Pressure_bar'] = np.nan\n",
      "\n",
      "# Coolant_Flow_L_min: solo per dispositivi con refrigerante\n",
      "df.loc[~df['Machine_Type'].isin(coolant_devices), 'Coolant_Flow_L_min'] = np.nan\n",
      "\n",
      "# Heat_Index: solo per dispositivi con calore\n",
      "df.loc[~df['Machine_Type'].isin(heat_devices), 'Heat_Index'] = np.nan\n",
      "\n",
      "# Per i dispositivi che dovrebbero avere queste features, imputa i valori mancanti\n",
      "# Usando la mediana per ogni gruppo di dispositivi\n",
      "\n",
      "for device_type in laser_devices:\n",
      "    mask = df['Machine_Type'] == device_type\n",
      "    if mask.sum() > 0 and 'Laser_Intensity' in df.columns:\n",
      "        median_val = df.loc[mask, 'Laser_Intensity'].median()\n",
      "        df.loc[mask & df['Laser_Intensity'].isna(), 'Laser_Intensity'] = median_val\n",
      "\n",
      "for device_type in hydraulic_devices:\n",
      "    mask = df['Machine_Type'] == device_type\n",
      "    if mask.sum() > 0 and 'Hydraulic_Pressure_bar' in df.columns:\n",
      "        median_val = df.loc[mask, 'Hydraulic_Pressure_bar'].median()\n",
      "        df.loc[mask & df['Hydraulic_Pressure_bar'].isna(), 'Hydraulic_Pressure_bar'] = median_val\n",
      "\n",
      "for device_type in coolant_devices:\n",
      "    mask = df['Machine_Type'] == device_type\n",
      "    if mask.sum() > 0 and 'Coolant_Flow_L_min' in df.columns:\n",
      "        median_val = df.loc[mask, 'Coolant_Flow_L_min'].median()\n",
      "        df.loc[mask & df['Coolant_Flow_L_min'].isna(), 'Coolant_Flow_L_min'] = median_val\n",
      "\n",
      "for device_type in heat_devices:\n",
      "    mask = df['Machine_Type'] == device_type\n",
      "    if mask.sum() > 0 and 'Heat_Index' in df.columns:\n",
      "        median_val = df.loc[mask, 'Heat_Index'].median()\n",
      "        df.loc[mask & df['Heat_Index'].isna(), 'Heat_Index'] = median_val\n",
      "\n",
      "print(\"‚úÖ Gestione valori mancanti completata\")\n",
      "\n",
      "# Controllo finale valori mancanti\n",
      "final_missing = df.isnull().sum()\n",
      "if final_missing.sum() > 0:\n",
      "    print(\"\\nValori mancanti rimanenti:\")\n",
      "    print(final_missing[final_missing > 0])\n",
      "else:\n",
      "    print(\"‚úÖ Nessun valore mancante rimanente!\")"
    ]
    },

    {
    "cell_type": "markdown",
    "metadata": {},
    "source": [
      "## 3. Feature Engineering"
    ]
    },

    {
    "cell_type": "code",
    "execution_count": null,
    "metadata": {},
    "outputs": [],
    "source": [
      "# Creazione nuove features derivate\n",
      "print(\"=== FEATURE ENGINEERING ===\")\n",
      "\n",
      "# 1. Et√† del dispositivo\n",
      "current_year = 2024\n",
      "df['Device_Age_Years'] = current_year - df['Installation_Year']\n",
      "\n",
      "# 2. Intensit√† di utilizzo\n",
      "df['Usage_Intensity'] = df['Operational_Hours'] / (df['Device_Age_Years'] * 365 * 24)\n",
      "df['Usage_Intensity'] = df['Usage_Intensity'].fillna(0)  # Per dispositivi installati quest'anno\n",
      "\n",
      "# 3. Rapporto manutenzioni/et√†\n",
      "df['Maintenance_Rate'] = df['Maintenance_History_Count'] / (df['Device_Age_Years'] + 1)  # +1 per evitare divisione per 0\n",
      "\n",
      "# 4. Rapporto guasti/et√†\n",
      "df['Failure_Rate'] = df['Failure_History_Count'] / (df['Device_Age_Years'] + 1)\n",
      "\n",
      "# 5. Indicatore manutenzione recente\n",
      "df['Recent_Maintenance'] = (df['Last_Maintenance_Days_Ago'] <= 30).astype(int)\n",
      "\n",
      "# 6. Indicatore alta temperatura\n",
      "df['High_Temperature'] = (df['Temperature_C'] > df['Temperature_C'].quantile(0.75)).astype(int)\n",
      "\n",
      "# 7. Indicatore alta vibrazione\n",
      "df['High_Vibration'] = (df['Vibration_mms'] > df['Vibration_mms'].quantile(0.75)).astype(int)\n",
      "\n",
      "# 8. Score di salute generale (combinazione di fattori)\n",
      "# Normalizzazione tra 0 e 1 per ogni componente\n",
      "temp_norm = (df['Temperature_C'] - df['Temperature_C'].min()) / (df['Temperature_C'].max() - df['Temperature_C'].min())\n",
      "vibration_norm = (df['Vibration_mms'] - df['Vibration_mms'].min()) / (df['Vibration_mms'].max() - df['Vibration_mms'].min())\n",
      "oil_norm = df['Oil_Level_pct'] / 100\n",
      "coolant_norm = df['Coolant_Level_pct'] / 100\n",
      "\n",
      "# Health score (pi√π alto = pi√π sano)\n",
      "df['Health_Score'] = ((1 - temp_norm) + (1 - vibration_norm) + oil_norm + coolant_norm) / 4\n",
      "\n",
      "# 9. Binning et√† dispositivo\n",
      "df['Age_Category'] = pd.cut(df['Device_Age_Years'], \n",
      "                           bins=[0, 2, 5, 10, float('inf')], \n",
      "                           labels=['Nuovo', 'Giovane', 'Maturo', 'Vecchio'])\n",
      "\n",
      "# 10. Interaction features\n",
      "df['Temp_Vibration_Interaction'] = df['Temperature_C'] * df['Vibration_mms']\n",
      "df['Age_Usage_Interaction'] = df['Device_Age_Years'] * df['Usage_Intensity']\n",
      "\n",
      "print(f\"‚úÖ Feature engineering completato. Nuove features create: {len(['Device_Age_Years', 'Usage_Intensity', 'Maintenance_Rate', 'Failure_Rate', 'Recent_Maintenance', 'High_Temperature', 'High_Vibration', 'Health_Score', 'Age_Category', 'Temp_Vibration_Interaction', 'Age_Usage_Interaction'])}\")\n",
      "print(f\"Forma dataset aggiornata: {df.shape}\")"
    ]
    },

    {
    "cell_type": "markdown",
    "metadata": {},
    "source": [
      "## 4. Gestione Outliers"
    ]
    },

    {
    "cell_type": "code",
    "execution_count": null,
    "metadata": {},
    "outputs": [],
    "source": [
      "# Identificazione e gestione outliers usando IQR\n",
      "def handle_outliers(data, column, method='cap'):\n",
      "    \"\"\"\n",
      "    Gestisce gli outliers usando il metodo IQR\n",
      "    method: 'cap' per cappare, 'remove' per rimuovere\n",
      "    \"\"\"\n",
      "    Q1 = data[column].quantile(0.25)\n",
      "    Q3 = data[column].quantile(0.75)\n",
      "    IQR = Q3 - Q1\n",
      "    lower_bound = Q1 - 1.5 * IQR\n",
      "    upper_bound = Q3 + 1.5 * IQR\n",
      "    \n",
      "    outliers_count = len(data[(data[column] < lower_bound) | (data[column] > upper_bound)])\n",
      "    \n",
      "    if method == 'cap':\n",
      "        data[column] = data[column].clip(lower_bound, upper_bound)\n",
      "    elif method == 'remove':\n",
      "        data = data[(data[column] >= lower_bound) & (data[column] <= upper_bound)]\n",
      "    \n",
      "    return data, outliers_count\n",
      "\n",
      "print(\"=== GESTIONE OUTLIERS ===\")\n",
      "\n",
      "# Features numeriche da processare (escludendo target e ID)\n",
      "numeric_features = df.select_dtypes(include=[np.number]).columns.tolist()\n",
      "features_to_process = [f for f in numeric_features if f not in ['Machine_ID', 'Failure_Within_7_Days', 'Remaining_Useful_Life_days']]\n",
      "\n",
      "outlier_summary = []\n",
      "\n",
      "for feature in features_to_process[:15]:  # Primi 15 per non sovraccaricare\n",
      "    df_processed, outliers_count = handle_outliers(df.copy(), feature, method='cap')\n",
      "    if outliers_count > 0:\n",
      "        df[feature] = df_processed[feature]\n",
      "        outlier_summary.append({\n",
      "            'Feature': feature,\n",
      "            'Outliers_Rimossi': outliers_count,\n",
      "            'Percentuale': (outliers_count / len(df)) * 100\n",
      "        })\n",
      "\n",
      "if outlier_summary:\n",
      "    outlier_df = pd.DataFrame(outlier_summary).sort_values('Percentuale', ascending=False)\n",
      "    display(outlier_df)\n",
      "    print(f\"\\n‚úÖ Outliers gestiti per {len(outlier_summary)} features\")\n",
      "else:\n",
      "    print(\"‚úÖ Nessun outlier significativo trovato\")"
    ]
    },

    {
    "cell_type": "markdown",
    "metadata": {},
    "source": [
      "## 5. Encoding Variabili Categoriche"
    ]
    },

    {
    "cell_type": "code",
    "execution_count": null,
    "metadata": {},
    "outputs": [],
    "source": [
      "# Encoding delle variabili categoriche\n",
      "print(\"=== ENCODING VARIABILI CATEGORICHE ===\")\n",
      "\n",
      "# Label Encoder per Machine_Type\n",
      "le_machine = LabelEncoder()\n",
      "df['Machine_Type_Encoded'] = le_machine.fit_transform(df['Machine_Type'])\n",
      "\n",
      "# Label Encoder per Device_Category\n",
      "le_category = LabelEncoder()\n",
      "df['Device_Category_Encoded'] = le_category.fit_transform(df['Device_Category'])\n",
      "\n",
      "# One-hot encoding per Age_Category\n",
      "age_dummies = pd.get_dummies(df['Age_Category'], prefix='Age')\n",
      "df = pd.concat([df, age_dummies], axis=1)\n",
      "\n",
      "print(f\"‚úÖ Encoding completato\")\n",
      "print(f\"Machine types unici: {df['Machine_Type'].nunique()}\")\n",
      "print(f\"Device categories: {df['Device_Category'].nunique()}\")\n",
      "print(f\"Nuove colonne age: {list(age_dummies.columns)}\")\n",
      "\n",
      "# Salvataggio encoders\n",
      "os.makedirs('../data/models', exist_ok=True)\n",
      "joblib.dump(le_machine, '../data/models/machine_type_encoder.pkl')\n",
      "joblib.dump(le_category, '../data/models/device_category_encoder.pkl')\n",
      "print(\"‚úÖ Encoders salvati\")"
    ]
    },

    {
    "cell_type": "markdown",
    "metadata": {},
    "source": [
      "## 6. Selezione Features Finali"
    ]
    },
    
    {
    "cell_type": "code",
    "execution_count": null,
    "metadata": {},
    "outputs": [],
    "source": [
      "# Definizione features finali per il modelling\n",
      "print(\"=== SELEZIONE FEATURES FINALI ===\")\n",
      "\n",
      "# Features comuni\n",
      "common_features = config['features']['common_features']\n",
      "\n",
      "# Features aggiuntive (sostituiamo NaN con 0 per modelling)\n",
      "additional_features = config['features']['additional_features']\n",
      "\n",
      "# Features engineered\n",
      "engineered_features = [\n",
      "    'Device_Age_Years', 'Usage_Intensity', 'Maintenance_Rate', 'Failure_Rate',\n",
      "    'Recent_Maintenance', 'High_Temperature', 'High_Vibration', 'Health_Score',\n",
      "    'Temp_Vibration_Interaction', 'Age_Usage_Interaction'\n",
      "]\n",
      "\n",
      "# Features encoded\n",
      "encoded_features = ['Machine_Type_Encoded', 'Device_Category_Encoded']\n",
      "\n",
      "# Age category dummies\n",
      "age_features = [col for col in df.columns if col.startswith('Age_')]\n",
      "\n",
      "# Combina tutte le features\n",
      "all_features = common_features + additional_features + engineered_features + encoded_features + age_features\n",
      "\n",
      "# Filtra solo le features che esistono nel dataframe\n",
      "available_features = [f for f in all_features if f in df.columns]\n",
      "\n",
      "print(f\"Features disponibili per il modelling: {len(available_features)}\")\n",
      "print(\"Features selezionate:\")\n",
      "for i, feature in enumerate(available_features, 1):\n",
      "    print(f\"  {i:2d}. {feature}\")\n",
      "\n",
      "# Riempimento NaN nelle additional features con 0 (per dispositivi che non le hanno)\n",
      "for feature in additional_features:\n",
      "    if feature in df.columns:\n",
      "        df[feature] = df[feature].fillna(0)\n",
      "\n",
      "# Verifica finale NaN\n",
      "final_missing = df[available_features].isnull().sum().sum()\n",
      "print(f\"\\nValori mancanti nelle features selezionate: {final_missing}\")"
    ]
    },

    {
    "cell_type": "markdown",
    "metadata": {},
    "source": [
      "## 7. Standardizzazione Features"
    ]
    },

    {
    "cell_type": "code",
    "execution_count": null,
    "metadata": {},
    "outputs": [],
    "source": [
      "# Preparazione dati per standardizzazione\n",
      "print(\"=== STANDARDIZZAZIONE FEATURES ===\")\n",
      "\n",
      "# Separazione features e target\n",
      "X = df[available_features].copy()\n",
      "y_classification = df['Failure_Within_7_Days'].copy()\n",
      "y_regression = df['Remaining_Useful_Life_days'].copy()\n",
      "\n",
      "print(f\"Forma X: {X.shape}\")\n",
      "print(f\"Forma y_classification: {y_classification.shape}\")\n",
      "print(f\"Forma y_regression: {y_regression.shape}\")\n",
      "\n",
      "# Identificazione features numeriche per standardizzazione\n",
      "numeric_features_to_scale = X.select_dtypes(include=[np.number]).columns.tolist()\n",
      "# Escludiamo le features gi√† binarie/categoriche encoded\n",
      "features_not_to_scale = ['Recent_Maintenance', 'High_Temperature', 'High_Vibration', \n",
      "                        'Machine_Type_Encoded', 'Device_Category_Encoded'] + age_features\n",
      "features_to_scale = [f for f in numeric_features_to_scale if f not in features_not_to_scale]\n",
      "\n",
      "print(f\"\\nFeatures da standardizzare: {len(features_to_scale)}\")\n",
      "print(f\"Features non standardizzate: {len(features_not_to_scale)}\")\n",
      "\n",
      "# Standardizzazione\n",
      "scaler = StandardScaler()\n",
      "X_scaled = X.copy()\n",
      "X_scaled[features_to_scale] = scaler.fit_transform(X[features_to_scale])\n",
      "\n",
      "# Salvataggio scaler\n",
      "joblib.dump(scaler, '../data/models/scaler.pkl')\n",
      "print(\"‚úÖ Scaler salvato\")\n",
      "\n",
      "# Verifica standardizzazione\n",
      "print(\"\\nStatistiche dopo standardizzazione (features scalate):\")\n",
      "print(f\"Media: {X_scaled[features_to_scale].mean().mean():.6f}\")\n",
      "print(f\"Std: {X_scaled[features_to_scale].std().mean():.6f}\")"
    ]
    },

    {
    "cell_type": "markdown",
    "metadata": {},
    "source": [
      "## 8. Divisione Train/Test Set"
    ]
    },

    {
    "cell_type": "code",
    "execution_count": null,
    "metadata": {},
    "outputs": [],
    "source": [
      "# Divisione train/test set\n",
      "print(\"=== DIVISIONE TRAIN/TEST SET ===\")\n",
      "\n",
      "test_size = config['model']['test_size']\n",
      "random_state = config['model']['random_state']\n",
      "\n",
      "# Stratified split basato sul target di classificazione\n",
      "X_train, X_test, y_class_train, y_class_test, y_reg_train, y_reg_test = train_test_split(\n",
      "    X_scaled, y_classification, y_regression,\n",
      "    test_size=test_size,\n",
      "    random_state=random_state,\n",
      "    stratify=y_classification\n",
      ")\n",
      "\n",
      "print(f\"Dimensioni training set: {X_train.shape}\")\n",
      "print(f\"Dimensioni test set: {X_test.shape}\")\n",
      "\n",
      "print(f\"\\nDistribuzione target classificazione - Train:\")\n",
      "print(y_class_train.value_counts(normalize=True).round(3))\n",
      "print(f\"\\nDistribuzione target classificazione - Test:\")\n",
      "print(y_class_test.value_counts(normalize=True).round(3))\n",
      "\n",
      "print(f\"\\nTarget regressione - Train stats:\")\n",
      "print(f\"  Media: {y_reg_train.mean():.2f}\")\n",
      "print(f\"  Std: {y_reg_train.std():.2f}\")\n",
      "print(f\"\\nTarget regressione - Test stats:\")\n",
      "print(f\"  Media: {y_reg_test.mean():.2f}\")\n",
      "print(f\"  Std: {y_reg_test.std():.2f}\")"
    ]
    },

    {
    "cell_type": "markdown",
    "metadata": {},
    "source": [
      "## 9. Salvataggio Dati Processati"
    ]
    },

    {
    "cell_type": "code",
    "execution_count": null,
    "metadata": {},
    "outputs": [],
    "source": [
      "# Salvataggio dati processati\n",
      "print(\"=== SALVATAGGIO DATI PROCESSATI ===\")\n",
      "\n",
      "# Creazione directory se non esiste\n",
      "processed_dir = '../data/processed/'\n",
      "os.makedirs(processed_dir, exist_ok=True)\n",
      "\n",
      "# Salvataggio dataset completo pulito\n",
      "df_clean = df.copy()\n",
      "df_clean.to_csv(processed_dir + 'cleaned_data.csv', index=False)\n",
      "print(f\"‚úÖ Dataset pulito salvato: {df_clean.shape}\")\n",
      "\n",
      "# Preparazione dati per salvataggio train/test\n",
      "# Combinazione features con target per i file di training\n",
      "train_data = X_train.copy()\n",
      "train_data['Failure_Within_7_Days'] = y_class_train.values\n",
      "train_data['Remaining_Useful_Life_days'] = y_reg_train.values\n",
      "test_data = X_test.copy()\n",
      "test_data['Failure_Within_7_Days'] = y_class_test.values\n",
      "test_data['Remaining_Useful_Life_days'] = y_reg_test.values\n",
      "\n",
      "# Salvataggio train/test sets\n",
      "train_data.to_csv(processed_dir + 'train_data.csv', index=False)\n",
      "test_data.to_csv(processed_dir + 'test_data.csv', index=False)\n",
      "print(f\"‚úÖ Train set salvato: {train_data.shape}\")\n",
      "print(f\"‚úÖ Test set salvato: {test_data.shape}\")\n",
      "\n",
      "# Salvataggio metadati del preprocessing\n",
      "preprocessing_metadata = {\n",
      "    'original_shape': list(df.shape),\n",
      "    'final_shape': list(X_scaled.shape),\n",
      "    'features_selected': available_features,\n",
      "    'features_to_scale': features_to_scale,\n",
      "    'features_not_to_scale': features_not_to_scale,\n",
      "    'test_size': test_size,\n",
      "    'random_state': random_state,\n",
      "    'target_distribution_train': y_class_train.value_counts().to_dict(),\n",
      "    'target_distribution_test': y_class_test.value_counts().to_dict()\n",
      "}\n",
      "\n",
      "import json\n",
      "with open(processed_dir + 'preprocessing_metadata.json', 'w') as f:\n",
      "    json.dump(preprocessing_metadata, f, indent=2)\n",
      "print(\"‚úÖ Metadati preprocessing salvati\")\n",
      "\n",
      "print(f\"\\nüéâ PREPROCESSING COMPLETATO CON SUCCESSO!\")\n",
      "print(f\"üìÅ File salvati in: {processed_dir}\")\n",
      "print(f\"üìä Dataset finale: {X_scaled.shape[0]} righe, {X_scaled.shape[1]} features\")\n",
      "print(f\"üéØ Target classificazione - Classe positiva: {y_classification.sum()} ({(y_classification.sum()/len(y_classification)*100):.1f}%)\")\n",
      "print(f\"üìà Target regressione - Range: {y_regression.min():.0f}-{y_regression.max():.0f} giorni\")\n"
      ]
    },

    {
    "cell_type": "markdown",
    "metadata": {},
    "source": [
      "## 10. Visualizzazioni Finali\n",
      "\n",
      "Alcune visualizzazioni per verificare la qualit√† del preprocessing.\n"
    ]
    },

    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Visualizzazioni Finali\n",
        "\n",
        "Alcune visualizzazioni per verificare la qualit√† del preprocessing."
      ]
    },

    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualizzazioni finali del preprocessing\n",
        "print(\"=== VISUALIZZAZIONI FINALI ===\")\n",
        "\n",
        "# 1. Distribuzione delle features pi√π importanti\n",
        "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
        "fig.suptitle('Distribuzione Features Principali (Dopo Preprocessing)', fontsize=16)\n",
        "\n",
        "# Top 6 features per visualizzazione\n",
        "top_features = ['Temperature_C', 'Vibration_mms', 'Device_Age_Years', \n",
        "               'Health_Score', 'Usage_Intensity', 'Power_Consumption_kW']\n",
        "\n",
        "for i, feature in enumerate(top_features):\n",
        "    row, col = i // 3, i % 3\n",
        "    if feature in X_scaled.columns:\n",
        "        axes[row, col].hist(X_scaled[feature], bins=50, alpha=0.7, color='skyblue', edgecolor='black')\n",
        "        axes[row, col].set_title(f'{feature}')\n",
        "        axes[row, col].set_xlabel('Valore')\n",
        "        axes[row, col].set_ylabel('Frequenza')\n",
        "        axes[row, col].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },

    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 2. Correlazione tra features principali e target\n",
        "correlation_features = ['Temperature_C', 'Vibration_mms', 'Device_Age_Years', \n",
        "                       'Health_Score', 'Last_Maintenance_Days_Ago', 'Failure_History_Count']\n",
        "\n",
        "# Creazione dataframe per correlazione\n",
        "corr_data = X_scaled[correlation_features].copy()\n",
        "corr_data['Failure_Within_7_Days'] = y_classification.values\n",
        "corr_data['Remaining_Useful_Life_days'] = y_regression.values\n",
        "\n",
        "plt.figure(figsize=(12, 8))\n",
        "correlation_matrix = corr_data.corr()\n",
        "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0,\n",
        "            square=True, linewidths=0.5)\n",
        "plt.title('Matrice di Correlazione - Features vs Targets')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },

    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 3. Distribuzione target\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
        "\n",
        "# Target classificazione\n",
        "y_classification.value_counts().plot(kind='bar', ax=ax1, color=['lightgreen', 'salmon'])\n",
        "ax1.set_title('Distribuzione Target Classificazione\\n(Failure_Within_7_Days)')\n",
        "ax1.set_xlabel('Classe')\n",
        "ax1.set_ylabel('Conteggio')\n",
        "ax1.set_xticklabels(['No Failure (0)', 'Failure (1)'], rotation=0)\n",
        "\n",
        "for i, v in enumerate(y_classification.value_counts()):\n",
        "    ax1.text(i, v + len(y_classification)*0.01, str(v), ha='center', va='bottom')\n",
        "\n",
        "# Target regressione\n",
        "ax2.hist(y_regression, bins=50, alpha=0.7, color='lightblue', edgecolor='black')\n",
        "ax2.set_title('Distribuzione Target Regressione\\n(Remaining_Useful_Life_days)')\n",
        "ax2.set_xlabel('Giorni Rimanenti')\n",
        "ax2.set_ylabel('Frequenza')\n",
        "ax2.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },

    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 4. Statistiche finali per categoria di dispositivo\n",
        "print(\"\\n=== STATISTICHE PER CATEGORIA DISPOSITIVO ===\")\n",
        "device_stats = df.groupby('Device_Category').agg({\n",
        "    'Failure_Within_7_Days': ['count', 'sum', 'mean'],\n",
        "    'Remaining_Useful_Life_days': ['mean', 'std'],\n",
        "    'Health_Score': 'mean',\n",
        "    'Device_Age_Years': 'mean'\n",
        "}).round(3)\n",
        "\n",
        "device_stats.columns = ['Count', 'Failures', 'Failure_Rate', 'Avg_Life_Days', \n",
        "                       'Std_Life_Days', 'Avg_Health_Score', 'Avg_Age_Years']\n",
        "display(device_stats)\n",
        "\n",
        "print(\"\\n‚úÖ Preprocessing completato e verificato!\")\n",
        "print(\"üìã Prossimi passi:\")\n",
        "print(\"   1. Eseguire model training (03_model_development.ipynb)\")\n",
        "print(\"   2. Valutare le performance (04_model_evaluation.ipynb)\")\n",
        "print(\"   3. Testare le predizioni (prediction_engine.py)\")"
      ]
    }
  ]
}