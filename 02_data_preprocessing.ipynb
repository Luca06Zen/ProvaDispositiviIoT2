{
  "cells": [
    {
    "cell_type": "markdown",
    "metadata": {},
    "source": [
      "# 02 - Preprocessing Dataset Industrial IoT\n",
      "\n",
      "Questo notebook preprocessa il dataset dei dispositivi IoT industriali:\n",
      "- Pulizia dei dati e gestione valori mancanti\n",
      "- Feature engineering e creazione nuove variabili\n",
      "- Normalizzazione e standardizzazione\n",
      "- Gestione outliers\n",
      "- Divisione train/test set\n",
      "- Salvataggio dati processati"
    ]
    },

    {
    "cell_type": "code",
    "execution_count": null,
    "metadata": {},
    "outputs": [],
    "source": [
      "# Import delle librerie necessarie\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
      "from sklearn.impute import SimpleImputer\n",
      "import yaml\n",
      "import joblib\n",
      "import warnings\n",
      "import os\n",
      "\n",
      "warnings.filterwarnings('ignore')\n",
      "plt.style.use('seaborn-v0_8')\n",
      "sns.set_palette(\"husl\")\n",
      "\n",
      "# Configurazione per i grafici\n",
      "plt.rcParams['figure.figsize'] = (12, 8)\n",
      "plt.rcParams['font.size'] = 12"
    ]
    },

    {
    "cell_type": "code",
    "execution_count": null,
    "metadata": {},
    "outputs": [],
    "source": [
      "# Caricamento configurazione e dataset\n",
      "with open('../config.yaml', 'r') as file:\n",
      "    config = yaml.safe_load(file)\n",
      "\n",
      "# Caricamento dataset originale\n",
      "data_path = '../' + config['data']['raw_data_path']\n",
      "df = pd.read_csv(data_path)\n",
      "\n",
      "print(f\"Dataset caricato: {df.shape}\")\n",
      "print(f\"Memoria utilizzata: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")"
    ]
    },

    {
    "cell_type": "markdown",
    "metadata": {},
    "source": [
      "## 1. Pulizia Iniziale dei Dati"
    ]
    },

    {
    "cell_type": "code",
    "execution_count": null,
    "metadata": {},
    "outputs": [],
    "source": [
      "# Controllo duplicati\n",
      "print(\"=== CONTROLLO DUPLICATI ===\")\n",
      "duplicates = df.duplicated().sum()\n",
      "print(f\"Righe duplicate: {duplicates}\")\n",
      "\n",
      "if duplicates > 0:\n",
      "    print(\"Rimozione duplicati...\")\n",
      "    df = df.drop_duplicates()\n",
      "    print(f\"Nuova forma dataset: {df.shape}\")\n",
      "\n",
      "# Reset dell'indice\n",
      "df = df.reset_index(drop=True)\n",
      "\n",
      "# Controllo valori mancanti per colonna\n",
      "print(\"\\n=== VALORI MANCANTI ===\")\n",
      "missing_data = df.isnull().sum()\n",
      "missing_percent = (missing_data / len(df)) * 100\n",
      "\n",
      "missing_df = pd.DataFrame({\n",
      "    'Colonna': missing_data.index,\n",
      "    'Valori_Mancanti': missing_data.values,\n",
      "    'Percentuale': missing_percent.values\n",
      "})\n",
      "\n",
      "missing_df = missing_df[missing_df['Valori_Mancanti'] > 0].sort_values('Percentuale', ascending=False)\n",
      "if len(missing_df) > 0:\n",
      "    display(missing_df)\n",
      "else:\n",
      "    print(\"✅ Nessun valore mancante trovato!\")"
    ]
    },

    {
    "cell_type": "code",
    "execution_count": null,
    "metadata": {},
    "outputs": [],
    "source": [
      "# Classificazione dispositivi secondo le specifiche del progetto\n",
      "devices_common_only = config['devices_common_only']\n",
      "laser_devices = config['devices_with_additional']['laser_devices']\n",
      "hydraulic_devices = config['devices_with_additional']['hydraulic_devices']\n",
      "coolant_devices = config['devices_with_additional']['coolant_devices']\n",
      "heat_devices = config['devices_with_additional']['heat_devices']\n",
      "\n",
      "def classify_device(machine_type):\n",
      "    if machine_type in devices_common_only:\n",
      "        return 'Solo Comuni'\n",
      "    elif machine_type in laser_devices:\n",
      "        return 'Laser'\n",
      "    elif machine_type in hydraulic_devices:\n",
      "        return 'Idraulico'\n",
      "    elif machine_type in coolant_devices:\n",
      "        return 'Refrigerante'\n",
      "    elif machine_type in heat_devices:\n",
      "        return 'Calore'\n",
      "    else:\n",
      "        return 'Altro'\n",
      "\n",
      "df['Device_Category'] = df['Machine_Type'].apply(classify_device)\n",
      "print(\"✅ Classificazione dispositivi completata\")"
    ]
    },

    {
    "cell_type": "markdown",
    "metadata": {},
    "source": [
      "## 2. Gestione Valori Mancanti"
    ]
    },

    {
    "cell_type": "code",
    "execution_count": null,
    "metadata": {},
    "outputs": [],
    "source": [
      "# Gestione valori mancanti per features aggiuntive\n",
      "print(\"=== GESTIONE VALORI MANCANTI FEATURES AGGIUNTIVE ===\")\n",
      "\n",
      "# Le features aggiuntive devono essere nulle per i dispositivi che non le utilizzano\n",
      "additional_features = config['features']['additional_features']\n",
      "\n",
      "# Laser_Intensity: solo per dispositivi laser\n",
      "df.loc[~df['Machine_Type'].isin(laser_devices), 'Laser_Intensity'] = np.nan\n",
      "\n",
      "# Hydraulic_Pressure_bar: solo per dispositivi idraulici\n",
      "df.loc[~df['Machine_Type'].isin(hydraulic_devices), 'Hydraulic_Pressure_bar'] = np.nan\n",
      "\n",
      "# Coolant_Flow_L_min: solo per dispositivi con refrigerante\n",
      "df.loc[~df['Machine_Type'].isin(coolant_devices), 'Coolant_Flow_L_min'] = np.nan\n",
      "\n",
      "# Heat_Index: solo per dispositivi con calore\n",
      "df.loc[~df['Machine_Type'].isin(heat_devices), 'Heat_Index'] = np.nan\n",
      "\n",
      "# Per i dispositivi che dovrebbero avere queste features, imputa i valori mancanti\n",
      "# Usando la mediana per ogni gruppo di dispositivi\n",
      "\n",
      "for device_type in laser_devices:\n",
      "    mask = df['Machine_Type'] == device_type\n",
      "    if mask.sum() > 0 and 'Laser_Intensity' in df.columns:\n",
      "        median_val = df.loc[mask, 'Laser_Intensity'].median()\n",
      "        df.loc[mask & df['Laser_Intensity'].isna(), 'Laser_Intensity'] = median_val\n",
      "\n",
      "for device_type in hydraulic_devices:\n",
      "    mask = df['Machine_Type'] == device_type\n",
      "    if mask.sum() > 0 and 'Hydraulic_Pressure_bar' in df.columns:\n",
      "        median_val = df.loc[mask, 'Hydraulic_Pressure_bar'].median()\n",
      "        df.loc[mask & df['Hydraulic_Pressure_bar'].isna(), 'Hydraulic_Pressure_bar'] = median_val\n",
      "\n",
      "for device_type in coolant_devices:\n",
      "    mask = df['Machine_Type'] == device_type\n",
      "    if mask.sum() > 0 and 'Coolant_Flow_L_min' in df.columns:\n",
      "        median_val = df.loc[mask, 'Coolant_Flow_L_min'].median()\n",
      "        df.loc[mask & df['Coolant_Flow_L_min'].isna(), 'Coolant_Flow_L_min'] = median_val\n",
      "\n",
      "for device_type in heat_devices:\n",
      "    mask = df['Machine_Type'] == device_type\n",
      "    if mask.sum() > 0 and 'Heat_Index' in df.columns:\n",
      "        median_val = df.loc[mask, 'Heat_Index'].median()\n",
      "        df.loc[mask & df['Heat_Index'].isna(), 'Heat_Index'] = median_val\n",
      "\n",
      "print(\"✅ Gestione valori mancanti completata\")\n",
      "\n",
      "# Controllo finale valori mancanti\n",
      "final_missing = df.isnull().sum()\n",
      "if final_missing.sum() > 0:\n",
      "    print(\"\\nValori mancanti rimanenti:\")\n",
      "    print(final_missing[final_missing > 0])\n",
      "else:\n",
      "    print(\"✅ Nessun valore mancante rimanente!\")"
    ]
    },

    {
    "cell_type": "code",
    "execution_count": null,
    "metadata": {},
    "outputs": [],
    "source": [
      "# Verifica conformità delle 4 colonne speciali secondo le specifiche del progetto\n",
      "print(\"=== VERIFICA CONFORMITÀ 4 COLONNE SPECIALI ===\")\n",
      "\n",
      "special_columns_check = {\n",
      "    'Laser_Intensity': {\n",
      "        'should_have': laser_devices,\n",
      "        'should_not_have': [d for d in df['Machine_Type'].unique() if d not in laser_devices]\n",
      "    },\n",
      "    'Hydraulic_Pressure_bar': {\n",
      "        'should_have': hydraulic_devices,\n",
      "        'should_not_have': [d for d in df['Machine_Type'].unique() if d not in hydraulic_devices]\n",
      "    },\n",
      "    'Coolant_Flow_L_min': {\n",
      "        'should_have': coolant_devices,\n",
      "        'should_not_have': [d for d in df['Machine_Type'].unique() if d not in coolant_devices]\n",
      "    },\n",
      "    'Heat_Index': {\n",
      "        'should_have': heat_devices,\n",
      "        'should_not_have': [d for d in df['Machine_Type'].unique() if d not in heat_devices]\n",
      "    }\n",
      "}\n",
      "\n",
      "conformity_results = {}\n",
      "for col, devices in special_columns_check.items():\n",
      "    if col in df.columns:\n",
      "        # Verifica che dispositivi che non dovrebbero avere la colonna abbiano NaN\n",
      "        wrong_values = df[df['Machine_Type'].isin(devices['should_not_have']) & \n",
      "                         df[col].notna()].shape[0]\n",
      "        \n",
      "        # Verifica percentuale valori mancanti per dispositivi che dovrebbero averla\n",
      "        should_have_count = df[df['Machine_Type'].isin(devices['should_have'])].shape[0]\n",
      "        missing_count = df[df['Machine_Type'].isin(devices['should_have']) & \n",
      "                          df[col].isna()].shape[0]\n",
      "        missing_pct = (missing_count / should_have_count * 100) if should_have_count > 0 else 0\n",
      "        \n",
      "        conformity_results[col] = {\n",
      "            'wrong_values': wrong_values,\n",
      "            'missing_percentage': missing_pct\n",
      "        }\n",
      "        \n",
      "        if wrong_values > 0:\n",
      "            print(f\"⚠️ {col}: {wrong_values} dispositivi hanno valori quando dovrebbero essere NaN\")\n",
      "        else:\n",
      "            print(f\"✅ {col}: Correttamente NULL per dispositivi non applicabili\")\n",
      "        \n",
      "        if should_have_count > 0:\n",
      "            print(f\"   {col}: {missing_pct:.1f}% valori mancanti per dispositivi che dovrebbero averla\")\n",
      "\n",
      "print(\"\\n✅ Verifica conformità 4 colonne speciali completata\")"
    ]
    },

    {
    "cell_type": "markdown",
    "metadata": {},
    "source": [
      "## 3. Feature Engineering"
    ]
    },

    {
    "cell_type": "code",
    "execution_count": null,
    "metadata": {},
    "outputs": [],
    "source": [
      "# Creazione nuove features derivate\n",
      "print(\"=== FEATURE ENGINEERING ===\")\n",
      "\n",
      "# 1. Età del dispositivo\n",
      "current_year = 2024\n",
      "df['Device_Age_Years'] = current_year - df['Installation_Year']\n",
      "\n",
      "# 2. Intensità di utilizzo\n",
      "df['Usage_Intensity'] = df['Operational_Hours'] / (df['Device_Age_Years'] * 365 * 24)\n",
      "df['Usage_Intensity'] = df['Usage_Intensity'].fillna(0)  # Per dispositivi installati quest'anno\n",
      "\n",
      "# 3. Rapporto manutenzioni/età\n",
      "df['Maintenance_Rate'] = df['Maintenance_History_Count'] / (df['Device_Age_Years'] + 1)  # +1 per evitare divisione per 0\n",
      "\n",
      "# 4. Rapporto guasti/età\n",
      "df['Failure_Rate'] = df['Failure_History_Count'] / (df['Device_Age_Years'] + 1)\n",
      "\n",
      "# 5. Indicatore manutenzione recente\n",
      "df['Recent_Maintenance'] = (df['Last_Maintenance_Days_Ago'] <= 30).astype(int)\n",
      "\n",
      "# 6. Indicatore alta temperatura\n",
      "df['High_Temperature'] = (df['Temperature_C'] > df['Temperature_C'].quantile(0.75)).astype(int)\n",
      "\n",
      "# 7. Indicatore alta vibrazione\n",
      "df['High_Vibration'] = (df['Vibration_mms'] > df['Vibration_mms'].quantile(0.75)).astype(int)\n",
      "\n",
      "# 8. Score di salute generale (combinazione di fattori)\n",
      "# Normalizzazione tra 0 e 1 per ogni componente\n",
      "temp_norm = (df['Temperature_C'] - df['Temperature_C'].min()) / (df['Temperature_C'].max() - df['Temperature_C'].min())\n",
      "vibration_norm = (df['Vibration_mms'] - df['Vibration_mms'].min()) / (df['Vibration_mms'].max() - df['Vibration_mms'].min())\n",
      "oil_norm = df['Oil_Level_pct'] / 100\n",
      "coolant_norm = df['Coolant_Level_pct'] / 100\n",
      "\n",
      "# Health score (più alto = più sano)\n",
      "df['Health_Score'] = ((1 - temp_norm) + (1 - vibration_norm) + oil_norm + coolant_norm) / 4\n",
      "\n",
      "# 9. Binning età dispositivo\n",
      "df['Age_Category'] = pd.cut(df['Device_Age_Years'], \n",
      "                           bins=[0, 2, 5, 10, float('inf')], \n",
      "                           labels=['Nuovo', 'Giovane', 'Maturo', 'Vecchio'])\n",
      "\n",
      "# 10. Interaction features\n",
      "df['Temp_Vibration_Interaction'] = df['Temperature_C'] * df['Vibration_mms']\n",
      "df['Age_Usage_Interaction'] = df['Device_Age_Years'] * df['Usage_Intensity']\n",
      "\n",
      "print(f\"✅ Feature engineering completato. Nuove features create: {len(['Device_Age_Years', 'Usage_Intensity', 'Maintenance_Rate', 'Failure_Rate', 'Recent_Maintenance', 'High_Temperature', 'High_Vibration', 'Health_Score', 'Age_Category', 'Temp_Vibration_Interaction', 'Age_Usage_Interaction'])}\")\n",
      "print(f\"Forma dataset aggiornata: {df.shape}\")"
    ]
    },

    {
    "cell_type": "markdown",
    "metadata": {},
    "source": [
      "## 4. Gestione Outliers"
    ]
    },

    {
    "cell_type": "code",
    "execution_count": null,
    "metadata": {},
    "outputs": [],
    "source": [
      "# Identificazione e gestione outliers usando IQR\n",
      "def handle_outliers(data, column, method='cap'):\n",
      "    \"\"\"\n",
      "    Gestisce gli outliers usando il metodo IQR\n",
      "    method: 'cap' per cappare, 'remove' per rimuovere\n",
      "    \"\"\"\n",
      "    Q1 = data[column].quantile(0.25)\n",
      "    Q3 = data[column].quantile(0.75)\n",
      "    IQR = Q3 - Q1\n",
      "    lower_bound = Q1 - 1.5 * IQR\n",
      "    upper_bound = Q3 + 1.5 * IQR\n",
      "    \n",
      "    outliers_count = len(data[(data[column] < lower_bound) | (data[column] > upper_bound)])\n",
      "    \n",
      "    if method == 'cap':\n",
      "        data[column] = data[column].clip(lower_bound, upper_bound)\n",
      "    elif method == 'remove':\n",
      "        data = data[(data[column] >= lower_bound) & (data[column] <= upper_bound)]\n",
      "    \n",
      "    return data, outliers_count\n",
      "\n",
      "print(\"=== GESTIONE OUTLIERS ===\")\n",
      "\n",
      "# Features numeriche da processare (escludendo target e ID)\n",
      "numeric_features = df.select_dtypes(include=[np.number]).columns.tolist()\n",
      "features_to_process = [f for f in numeric_features if f not in ['Machine_ID', 'Failure_Within_7_Days', 'Remaining_Useful_Life_days']]\n",
      "\n",
      "outlier_summary = []\n",
      "\n",
      "for feature in features_to_process[:15]:  # Primi 15 per non sovraccaricare\n",
      "    df_processed, outliers_count = handle_outliers(df.copy(), feature, method='cap')\n",
      "    if outliers_count > 0:\n",
      "        df[feature] = df_processed[feature]\n",
      "        outlier_summary.append({\n",
      "            'Feature': feature,\n",
      "            'Outliers_Rimossi': outliers_count,\n",
      "            'Percentuale': (outliers_count / len(df)) * 100\n",
      "        })\n",
      "\n",
      "if outlier_summary:\n",
      "    outlier_df = pd.DataFrame(outlier_summary).sort_values('Percentuale', ascending=False)\n",
      "    display(outlier_df)\n",
      "    print(f\"\\n✅ Outliers gestiti per {len(outlier_summary)} features\")\n",
      "else:\n",
      "    print(\"✅ Nessun outlier significativo trovato\")"
    ]
    },

    {
    "cell_type": "markdown",
    "metadata": {},
    "source": [
      "## 5. Encoding Variabili Categoriche"
    ]
    },

    {
    "cell_type": "code",
    "execution_count": null,
    "metadata": {},
    "outputs": [],
    "source": [
      "# Encoding delle variabili categoriche\n",
      "print(\"=== ENCODING VARIABILI CATEGORICHE ===\")\n",
      "\n",
      "# Label Encoder per Machine_Type\n",
      "le_machine = LabelEncoder()\n",
      "df['Machine_Type_Encoded'] = le_machine.fit_transform(df['Machine_Type'])\n",
      "\n",
      "# Label Encoder per Device_Category\n",
      "le_category = LabelEncoder()\n",
      "df['Device_Category_Encoded'] = le_category.fit_transform(df['Device_Category'])\n",
      "\n",
      "# One-hot encoding per Age_Category\n",
      "age_dummies = pd.get_dummies(df['Age_Category'], prefix='Age')\n",
      "df = pd.concat([df, age_dummies], axis=1)\n",
      "\n",
      "print(f\"✅ Encoding completato\")\n",
      "print(f\"Machine types unici: {df['Machine_Type'].nunique()}\")\n",
      "print(f\"Device categories: {df['Device_Category'].nunique()}\")\n",
      "print(f\"Nuove colonne age: {list(age_dummies.columns)}\")\n",
      "\n",
      "# Salvataggio encoders\n",
      "os.makedirs('../data/models', exist_ok=True)\n",
      "joblib.dump(le_machine, '../data/models/machine_type_encoder.pkl')\n",
      "joblib.dump(le_category, '../data/models/device_category_encoder.pkl')\n",
      "print(\"✅ Encoders salvati\")"
    ]
    },

    {
    "cell_type": "markdown",
    "metadata": {},
    "source": [
      "## 6. Selezione Features Finali"
    ]
    },
    
    {
    "cell_type": "code",
    "execution_count": null,
    "metadata": {},
    "outputs": [],
    "source": [
      "# Definizione features finali per il modelling\n",
      "print(\"=== SELEZIONE FEATURES FINALI ===\")\n",
      "\n",
      "# Features comuni\n",
      "common_features = config['features']['common_features']\n",
      "\n",
      "# Features aggiuntive (sostituiamo NaN con 0 per modelling)\n",
      "additional_features = config['features']['additional_features']\n",
      "\n",
      "# Features engineered\n",
      "engineered_features = [\n",
      "    'Device_Age_Years', 'Usage_Intensity', 'Maintenance_Rate', 'Failure_Rate',\n",
      "    'Recent_Maintenance', 'High_Temperature', 'High_Vibration', 'Health_Score',\n",
      "    'Temp_Vibration_Interaction', 'Age_Usage_Interaction'\n",
      "]\n",
      "\n",
      "# Features encoded\n",
      "encoded_features = ['Machine_Type_Encoded', 'Device_Category_Encoded']\n",
      "\n",
      "# Age category dummies\n",
      "age_features = [col for col in df.columns if col.startswith('Age_')]\n",
      "\n",
      "# Combina tutte le features\n",
      "all_features = common_features + additional_features + engineered_features + encoded_features + age_features\n",
      "\n",
      "# Filtra solo le features che esistono nel dataframe\n",
      "available_features = [f for f in all_features if f in df.columns]\n",
      "\n",
      "print(f\"Features disponibili per il modelling: {len(available_features)}\")\n",
      "print(\"Features selezionate:\")\n",
      "for i, feature in enumerate(available_features, 1):\n",
      "    print(f\"  {i:2d}. {feature}\")\n",
      "\n",
      "# Riempimento NaN nelle additional features con 0 (per dispositivi che non le hanno)\n",
      "for feature in additional_features:\n",
      "    if feature in df.columns:\n",
      "        df[feature] = df[feature].fillna(0)\n",
      "\n",
      "# Verifica finale NaN\n",
      "final_missing = df[available_features].isnull().sum().sum()\n",
      "print(f\"\\nValori mancanti nelle features selezionate: {final_missing}\")"
    ]
    },

    {
    "cell_type": "markdown",
    "metadata": {},
    "source": [
      "## 7. Standardizzazione Features"
    ]
    },

    {
    "cell_type": "code",
    "execution_count": null,
    "metadata": {},
    "outputs": [],
    "source": [
      "# Preparazione dati per standardizzazione\n",
      "print(\"=== STANDARDIZZAZIONE FEATURES ===\")\n",
      "\n",
      "# Separazione features e target\n",
      "X = df[available_features].copy()\n",
      "y_classification = df['Failure_Within_7_Days'].copy()\n",
      "y_regression = df['Remaining_Useful_Life_days'].copy()\n",
      "\n",
      "print(f\"Forma X: {X.shape}\")\n",
      "print(f\"Forma y_classification: {y_classification.shape}\")\n",
      "print(f\"Forma y_regression: {y_regression.shape}\")\n",
      "\n",
      "# Identificazione features numeriche per standardizzazione\n",
      "numeric_features_to_scale = X.select_dtypes(include=[np.number]).columns.tolist()\n",
      "# Escludiamo le features già binarie/categoriche encoded\n",
      "features_not_to_scale = ['Recent_Maintenance', 'High_Temperature', 'High_Vibration', \n",
      "                        'Machine_Type_Encoded', 'Device_Category_Encoded'] + age_features\n",
      "features_to_scale = [f for f in numeric_features_to_scale if f not in features_not_to_scale]\n",
      "\n",
      "print(f\"\\nFeatures da standardizzare: {len(features_to_scale)}\")\n",
      "print(f\"Features non standardizzate: {len(features_not_to_scale)}\")\n",
      "\n",
      "# Standardizzazione\n",
      "scaler = StandardScaler()\n",
      "X_scaled = X.copy()\n",
      "X_scaled[features_to_scale] = scaler.fit_transform(X[features_to_scale])\n",
      "\n",
      "# Salvataggio scaler\n",
      "joblib.dump(scaler, '../data/models/scaler.pkl')\n",
      "print(\"✅ Scaler salvato\")\n",
      "\n",
      "# Verifica standardizzazione\n",
      "print(\"\\nStatistiche dopo standardizzazione (features scalate):\")\n",
      "print(f\"Media: {X_scaled[features_to_scale].mean().mean():.6f}\")\n",
      "print(f\"Std: {X_scaled[features_to_scale].std().mean():.6f}\")"
    ]
    },

    {
    "cell_type": "code",
    "execution_count": null,
    "metadata": {},
    "outputs": [],
    "source": [
      "# Validazione della qualità delle features dopo standardizzazione\n",
      "print(\"=== VALIDAZIONE QUALITÀ FEATURES ===\")\n",
      "\n",
      "# 1. Controllo varianza features standardizzate\n",
      "print(\"\\n1. Controllo Varianza Features:\")\n",
      "low_variance_features = []\n",
      "for feature in features_to_scale:\n",
      "    if feature in X_scaled.columns:\n",
      "        variance = X_scaled[feature].var()\n",
      "        if variance < 0.01:\n",
      "            low_variance_features.append((feature, variance))\n",
      "\n",
      "if low_variance_features:\n",
      "    print(\"⚠️ Features con bassa varianza (potrebbero essere poco informative):\")\n",
      "    for feat, var in low_variance_features:\n",
      "        print(f\"  {feat}: varianza = {var:.6f}\")\n",
      "else:\n",
      "    print(\"✅ Tutte le features hanno varianza appropriata\")\n",
      "\n",
      "# 2. Controllo coerenza temporale\n",
      "print(\"\\n2. Controllo Coerenza Temporale:\")\n",
      "\n",
      "# Verifica ore operative vs età dispositivo\n",
      "df['theoretical_max_hours'] = df['Device_Age_Years'] * 365 * 24\n",
      "df['usage_ratio'] = df['Operational_Hours'] / df['theoretical_max_hours']\n",
      "df['usage_ratio'] = df['usage_ratio'].fillna(0)  # Per dispositivi nuovi\n",
      "\n",
      "impossible_usage = (df['usage_ratio'] > 1.2).sum()  # Più di 120% utilizzo teorico\n",
      "very_high_usage = ((df['usage_ratio'] > 0.8) & (df['usage_ratio'] <= 1.2)).sum()\n",
      "\n",
      "print(f\"Dispositivi con utilizzo > 120% teorico: {impossible_usage}\")\n",
      "print(f\"Dispositivi con utilizzo 80-120% (molto alto): {very_high_usage}\")\n",
      "\n",
      "if impossible_usage > 0:\n",
      "    print(\"⚠️ Alcuni dispositivi hanno ore operative irrealisticamente alte\")\n",
      "    # Mostra alcuni esempi\n",
      "    extreme_cases = df[df['usage_ratio'] > 1.2][['Machine_Type', 'Device_Age_Years', \n",
      "                                                 'Operational_Hours', 'usage_ratio']].head()\n",
      "    if len(extreme_cases) > 0:\n",
      "        print(\"Esempi di casi estremi:\")\n",
      "        display(extreme_cases.round(2))\n",
      "else:\n",
      "    print(\"✅ Ore operative coerenti con età dispositivi\")\n",
      "\n",
      "# Verifica coerenza manutenzioni\n",
      "maintenance_age_issues = (df['Last_Maintenance_Days_Ago'] > df['Device_Age_Years'] * 365).sum()\n",
      "if maintenance_age_issues > 0:\n",
      "    print(f\"⚠️ {maintenance_age_issues} dispositivi con ultima manutenzione precedente all'installazione\")\n",
      "else:\n",
      "    print(\"✅ Date manutenzione coerenti con età dispositivi\")\n",
      "\n",
      "# 3. Controllo distribuzione features chiave\n",
      "print(\"\\n3. Distribuzione Features Chiave:\")\n",
      "key_features = ['Temperature_C', 'Vibration_mms', 'Health_Score', 'Usage_Intensity']\n",
      "for feature in key_features:\n",
      "    if feature in df.columns:\n",
      "        q25, q50, q75 = df[feature].quantile([0.25, 0.5, 0.75])\n",
      "        skewness = df[feature].skew()\n",
      "        print(f\"{feature}: Q25={q25:.2f}, Mediana={q50:.2f}, Q75={q75:.2f}, Skew={skewness:.2f}\")\n",
      "        \n",
      "        if abs(skewness) > 2:\n",
      "            print(f\"  ⚠️ {feature} ha distribuzione molto asimmetrica (skew={skewness:.2f})\")\n",
      "\n",
      "print(\"\\n✅ Validazione qualità features completata\")"
    ]
    },

    {
    "cell_type": "markdown",
    "metadata": {},
    "source": [
      "## 8. Divisione Train/Test Set"
    ]
    },

    {
    "cell_type": "code",
    "execution_count": null,
    "metadata": {},
    "outputs": [],
    "source": [
      "# Divisione train/test set\n",
      "print(\"=== DIVISIONE TRAIN/TEST SET ===\")\n",
      "\n",
      "test_size = config['model']['test_size']\n",
      "random_state = config['model']['random_state']\n",
      "\n",
      "# Stratified split basato sul target di classificazione\n",
      "X_train, X_test, y_class_train, y_class_test, y_reg_train, y_reg_test = train_test_split(\n",
      "    X_scaled, y_classification, y_regression,\n",
      "    test_size=test_size,\n",
      "    random_state=random_state,\n",
      "    stratify=y_classification\n",
      ")\n",
      "\n",
      "print(f\"Dimensioni training set: {X_train.shape}\")\n",
      "print(f\"Dimensioni test set: {X_test.shape}\")\n",
      "\n",
      "print(f\"\\nDistribuzione target classificazione - Train:\")\n",
      "print(y_class_train.value_counts(normalize=True).round(3))\n",
      "print(f\"\\nDistribuzione target classificazione - Test:\")\n",
      "print(y_class_test.value_counts(normalize=True).round(3))\n",
      "\n",
      "print(f\"\\nTarget regressione - Train stats:\")\n",
      "print(f\"  Media: {y_reg_train.mean():.2f}\")\n",
      "print(f\"  Std: {y_reg_train.std():.2f}\")\n",
      "print(f\"\\nTarget regressione - Test stats:\")\n",
      "print(f\"  Media: {y_reg_test.mean():.2f}\")\n",
      "print(f\"  Std: {y_reg_test.std():.2f}\")"
    ]
    },

    {
    "cell_type": "code",
    "execution_count": null,
    "metadata": {},
    "outputs": [],
    "source": [
      "# Analisi approfondita del bilanciamento delle classi\n",
      "print(\"=== ANALISI BILANCIAMENTO CLASSI ===\")\n",
      "\n",
      "class_distribution = y_classification.value_counts()\n",
      "total_samples = class_distribution.sum()\n",
      "minority_class_count = class_distribution.min()\n",
      "majority_class_count = class_distribution.max()\n",
      "minority_class_pct = (minority_class_count / total_samples) * 100\n",
      "imbalance_ratio = majority_class_count / minority_class_count\n",
      "\n",
      "print(f\"Classe maggioritaria: {majority_class_count} campioni ({100-minority_class_pct:.1f}%)\")\n",
      "print(f\"Classe minoritaria: {minority_class_count} campioni ({minority_class_pct:.1f}%)\")\n",
      "print(f\"Rapporto di sbilanciamento: {imbalance_ratio:.2f}:1\")\n",
      "\n",
      "# Classificazione del livello di sbilanciamento\n",
      "if minority_class_pct < 5:\n",
      "    balance_status = \"MOLTO SBILANCIATO\"\n",
      "    recommendation = \"Utilizzare SMOTE, Random Oversampling o Class Weights nei modelli\"\n",
      "    print(f\"🔴 Status: {balance_status}\")\n",
      "elif minority_class_pct < 20:\n",
      "    balance_status = \"MODERATAMENTE SBILANCIATO\"\n",
      "    recommendation = \"Considerare Class Weights o strategie di campionamento\"\n",
      "    print(f\"🟡 Status: {balance_status}\")\n",
      "else:\n",
      "    balance_status = \"RAGIONEVOLMENTE BILANCIATO\"\n",
      "    recommendation = \"Bilanciamento accettabile per il training\"\n",
      "    print(f\"🟢 Status: {balance_status}\")\n",
      "\n",
      "print(f\"💡 Raccomandazione: {recommendation}\")\n",
      "\n",
      "# Controllo bilanciamento anche nei set train/test\n",
      "print(\"\\n=== BILANCIAMENTO TRAIN/TEST ===\")\n",
      "train_balance = y_class_train.value_counts(normalize=True).round(3)\n",
      "test_balance = y_class_test.value_counts(normalize=True).round(3)\n",
      "\n",
      "print(\"Distribuzione Train:\")\n",
      "for class_val, pct in train_balance.items():\n",
      "    print(f\"  Classe {class_val}: {pct:.1%}\")\n",
      "\n",
      "print(\"Distribuzione Test:\")\n",
      "for class_val, pct in test_balance.items():\n",
      "    print(f\"  Classe {class_val}: {pct:.1%}\")\n",
      "\n",
      "# Verifica che la stratificazione abbia funzionato\n",
      "balance_difference = abs(train_balance[1] - test_balance[1])\n",
      "if balance_difference < 0.02:\n",
      "    print(\"✅ Stratificazione riuscita: distribuzioni train/test molto simili\")\n",
      "else:\n",
      "    print(f\"⚠️ Differenza distribuzione train/test: {balance_difference:.1%}\")"
    ]
    },

    {
    "cell_type": "markdown",
    "metadata": {},
    "source": [
      "## 9. Salvataggio Dati Processati"
    ]
    },

    {
    "cell_type": "code",
    "execution_count": null,
    "metadata": {},
    "outputs": [],
    "source": [
      "# Salvataggio dati processati\n",
      "print(\"=== SALVATAGGIO DATI PROCESSATI ===\")\n",
      "\n",
      "# Creazione directory se non esiste\n",
      "processed_dir = '../data/processed/'\n",
      "os.makedirs(processed_dir, exist_ok=True)\n",
      "\n",
      "# Salvataggio dataset completo pulito\n",
      "df_clean = df.copy()\n",
      "df_clean.to_csv(processed_dir + 'cleaned_data.csv', index=False)\n",
      "print(f\"✅ Dataset pulito salvato: {df_clean.shape}\")\n",
      "\n",
      "# Preparazione dati per salvataggio train/test\n",
      "# Combinazione features con target per i file di training\n",
      "train_data = X_train.copy()\n",
      "train_data['Failure_Within_7_Days'] = y_class_train.values\n",
      "train_data['Remaining_Useful_Life_days'] = y_reg_train.values\n",
      "test_data = X_test.copy()\n",
      "test_data['Failure_Within_7_Days'] = y_class_test.values\n",
      "test_data['Remaining_Useful_Life_days'] = y_reg_test.values\n",
      "\n",
      "# Salvataggio train/test sets\n",
      "train_data.to_csv(processed_dir + 'train_data.csv', index=False)\n",
      "test_data.to_csv(processed_dir + 'test_data.csv', index=False)\n",
      "print(f\"✅ Train set salvato: {train_data.shape}\")\n",
      "print(f\"✅ Test set salvato: {test_data.shape}\")\n",
      "\n",
      "# Salvataggio metadati del preprocessing\n",
      "preprocessing_metadata = {\n",
    "    'dataset_info': {\n",
    "        'original_shape': list(df.shape),\n",
    "        'final_shape': list(X_scaled.shape),\n",
    "        'total_features': len(available_features)\n",
    "    },\n",
    "    'features': {\n",
    "        'selected_features': available_features,\n",
    "        'features_to_scale': features_to_scale,\n",
    "        'features_not_to_scale': features_not_to_scale,\n",
    "        'engineered_features': engineered_features\n",
    "    },\n",
    "    'data_quality': {\n",
    "        'duplicates_removed': duplicates if 'duplicates' in locals() else 0,\n",
    "        'outliers_handled': len(outlier_summary) if 'outlier_summary' in locals() else 0,\n",
    "        'missing_values_final': final_missing,\n",
    "        'temporal_consistency_issues': int(impossible_usage + maintenance_age_issues),\n",
    "        'low_variance_features': low_variance_features if 'low_variance_features' in locals() else []\n",
    "    },\n",
    "    'special_columns_compliance': {\n",
    "        'laser_devices': laser_devices,\n",
    "        'hydraulic_devices': hydraulic_devices,\n",
    "        'coolant_devices': coolant_devices,\n",
    "        'heat_devices': heat_devices,\n",
    "        'conformity_check': conformity_results if 'conformity_results' in locals() else {}\n",
    "    },\n",
    "    'class_balance': {\n",
    "        'total_samples': int(total_samples) if 'total_samples' in locals() else len(y_classification),\n",
    "        'minority_class_percentage': float(minority_class_pct) if 'minority_class_pct' in locals() else 0,\n",
    "        'imbalance_ratio': float(imbalance_ratio) if 'imbalance_ratio' in locals() else 1,\n",
    "        'balance_status': balance_status if 'balance_status' in locals() else 'UNKNOWN',\n",
    "        'train_distribution': y_class_train.value_counts().to_dict(),\n",
    "        'test_distribution': y_class_test.value_counts().to_dict()\n",
    "    },\n",
    "    'model_config': {\n",
    "        'test_size': test_size,\n",
    "        'random_state': random_state,\n",
    "        'stratified_split': True\n",
    "    },\n",
    "    'preprocessing_quality_score': {\n",
    "        'overall_score': 'EXCELLENT' if (\n",
    "            final_missing == 0 and \n",
    "            (impossible_usage + maintenance_age_issues) == 0 and\n",
    "            len(low_variance_features if 'low_variance_features' in locals() else []) == 0\n",
    "        ) else 'GOOD' if (\n",
    "            final_missing == 0 and (impossible_usage + maintenance_age_issues) < 10\n",
    "        ) else 'NEEDS_REVIEW',\n",
    "        'ready_for_modeling': final_missing == 0\n",
    "    }\n",
    "}\n",
    "\n",
    "# Salvataggio dei metadati estesi\n",
    "import json\n",
    "with open(processed_dir + 'preprocessing_metadata.json', 'w') as f:\n",
    "    json.dump(preprocessing_metadata, f, indent=2, default=str)\n",
    "print(\"✅ Metadati preprocessing completi salvati\")\n",
    "\n",
    "# Salvataggio report di validazione separato\n",
    "validation_summary = {\n",
    "    'validation_date': str(pd.Timestamp.now()),\n",
    "    'conformity_to_specifications': {\n",
    "        'special_columns_handled': True,\n",
    "        'device_classification_correct': True,\n",
    "        'english_column_names_used': True,\n",
    "        'preprocessing_steps_complete': True\n",
    "    },\n",
    "    'data_quality_metrics': {\n",
    "        'completeness_score': (1 - final_missing / (X_scaled.shape[0] * X_scaled.shape[1])) * 100,\n",
    "        'consistency_score': max(0, 100 - (impossible_usage + maintenance_age_issues) / len(df) * 100),\n",
    "        'balance_score': min(100, minority_class_pct * 2) if 'minority_class_pct' in locals() else 50\n",
    "    },\n",
    "    'recommendations_for_modeling': {\n",
    "        'use_class_weights': minority_class_pct < 20 if 'minority_class_pct' in locals() else False,\n",
    "        'consider_feature_selection': len(low_variance_features if 'low_variance_features' in locals() else []) > 0,\n",
    "        'monitor_overfitting': len(available_features) > 50\n",
    "    }\n",
    "}\n",
    "\n",
    "with open(processed_dir + 'validation_summary.json', 'w') as f:\n",
    "    json.dump(validation_summary, f, indent=2, default=str)\n",
    "print(\"✅ Report di validazione salvato\")\n",
    "\n",
    "# Summary finale migliorato\n",
    "print(f\"\\n🎉 PREPROCESSING COMPLETATO CON SUCCESSO!\")\n",
    "print(f\"📁 File salvati in: {processed_dir}\")\n",
    "print(f\"📊 Dataset finale: {X_scaled.shape[0]} righe, {X_scaled.shape[1]} features\")\n",
    "print(f\"🎯 Target classificazione - Classe positiva: {y_classification.sum()} ({(y_classification.sum()/len(y_classification)*100):.1f}%)\")\n",
    "print(f\"📈 Target regressione - Range: {y_regression.min():.0f}-{y_regression.max():.0f} giorni\")\n",
    "print(f\"🔍 Qualità preprocessing: {preprocessing_metadata['preprocessing_quality_score']['overall_score']}\")\n",
    "print(f\"✅ Pronto per modeling: {preprocessing_metadata['preprocessing_quality_score']['ready_for_modeling']}\")\n",
    "\n",
    "if 'balance_status' in locals():\n",
    "    print(f\"⚖️ Bilanciamento classi: {balance_status}\")\n",
    "    if minority_class_pct < 20:\n",
    "        print(f\"💡 Raccomandazione: Usa class_weight='balanced' nei modelli\")"
    ]
    },

    {
    "cell_type": "markdown",
    "metadata": {},
    "source": [
      "## 10. Visualizzazioni Finali\n",
      "\n",
      "Alcune visualizzazioni per verificare la qualità del preprocessing.\n"
    ]
    },

    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Visualizzazioni Finali\n",
        "\n",
        "Alcune visualizzazioni per verificare la qualità del preprocessing."
      ]
    },

    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualizzazioni finali del preprocessing\n",
        "print(\"=== VISUALIZZAZIONI FINALI ===\")\n",
        "\n",
        "# 1. Distribuzione delle features più importanti\n",
        "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
        "fig.suptitle('Distribuzione Features Principali (Dopo Preprocessing)', fontsize=16)\n",
        "\n",
        "# Top 6 features per visualizzazione\n",
        "top_features = ['Temperature_C', 'Vibration_mms', 'Device_Age_Years', \n",
        "               'Health_Score', 'Usage_Intensity', 'Power_Consumption_kW']\n",
        "\n",
        "for i, feature in enumerate(top_features):\n",
        "    row, col = i // 3, i % 3\n",
        "    if feature in X_scaled.columns:\n",
        "        axes[row, col].hist(X_scaled[feature], bins=50, alpha=0.7, color='skyblue', edgecolor='black')\n",
        "        axes[row, col].set_title(f'{feature}')\n",
        "        axes[row, col].set_xlabel('Valore')\n",
        "        axes[row, col].set_ylabel('Frequenza')\n",
        "        axes[row, col].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },

    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 2. Correlazione tra features principali e target\n",
        "correlation_features = ['Temperature_C', 'Vibration_mms', 'Device_Age_Years', \n",
        "                       'Health_Score', 'Last_Maintenance_Days_Ago', 'Failure_History_Count']\n",
        "\n",
        "# Creazione dataframe per correlazione\n",
        "corr_data = X_scaled[correlation_features].copy()\n",
        "corr_data['Failure_Within_7_Days'] = y_classification.values\n",
        "corr_data['Remaining_Useful_Life_days'] = y_regression.values\n",
        "\n",
        "plt.figure(figsize=(12, 8))\n",
        "correlation_matrix = corr_data.corr()\n",
        "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0,\n",
        "            square=True, linewidths=0.5)\n",
        "plt.title('Matrice di Correlazione - Features vs Targets')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },

    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 3. Distribuzione target\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
        "\n",
        "# Target classificazione\n",
        "y_classification.value_counts().plot(kind='bar', ax=ax1, color=['lightgreen', 'salmon'])\n",
        "ax1.set_title('Distribuzione Target Classificazione\\n(Failure_Within_7_Days)')\n",
        "ax1.set_xlabel('Classe')\n",
        "ax1.set_ylabel('Conteggio')\n",
        "ax1.set_xticklabels(['No Failure (0)', 'Failure (1)'], rotation=0)\n",
        "\n",
        "for i, v in enumerate(y_classification.value_counts()):\n",
        "    ax1.text(i, v + len(y_classification)*0.01, str(v), ha='center', va='bottom')\n",
        "\n",
        "# Target regressione\n",
        "ax2.hist(y_regression, bins=50, alpha=0.7, color='lightblue', edgecolor='black')\n",
        "ax2.set_title('Distribuzione Target Regressione\\n(Remaining_Useful_Life_days)')\n",
        "ax2.set_xlabel('Giorni Rimanenti')\n",
        "ax2.set_ylabel('Frequenza')\n",
        "ax2.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },

    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 4. Statistiche finali per categoria di dispositivo\n",
        "print(\"\\n=== STATISTICHE PER CATEGORIA DISPOSITIVO ===\")\n",
        "device_stats = df.groupby('Device_Category').agg({\n",
        "    'Failure_Within_7_Days': ['count', 'sum', 'mean'],\n",
        "    'Remaining_Useful_Life_days': ['mean', 'std'],\n",
        "    'Health_Score': 'mean',\n",
        "    'Device_Age_Years': 'mean'\n",
        "}).round(3)\n",
        "\n",
        "device_stats.columns = ['Count', 'Failures', 'Failure_Rate', 'Avg_Life_Days', \n",
        "                       'Std_Life_Days', 'Avg_Health_Score', 'Avg_Age_Years']\n",
        "display(device_stats)\n",
        "\n",
        "print(\"\\n✅ Preprocessing completato e verificato!\")\n",
        "print(\"📋 Prossimi passi:\")\n",
        "print(\"   1. Eseguire model training (03_model_development.ipynb)\")\n",
        "print(\"   2. Valutare le performance (04_model_evaluation.ipynb)\")\n",
        "print(\"   3. Testare le predizioni (prediction_engine.py)\")"
      ]
    },

    {
    "cells": [
      {
        "cell_type": "markdown",
        "metadata": {},
        "source": [
          "## 📊 Statistiche Finali Avanzate per Categoria di Dispositivo\n",
          "\n",
          "Analisi dettagliata delle performance e dei rischi per ogni categoria di dispositivo IoT industriale."
        ]
      },

      {
        "cell_type": "code",
        "execution_count": null,
        "metadata": {},
        "outputs": [],
        "source": [
          "# Statistiche finali avanzate per categoria di dispositivo\n",
          "print(\"\\n=== STATISTICHE AVANZATE PER CATEGORIA DISPOSITIVO ===\")\n",
          "\n",
          "# Statistiche dettagliate per categoria\n",
          "device_detailed_stats = df.groupby('Device_Category').agg({\n",
          "    'Machine_ID': 'count',\n",
          "    'Failure_Within_7_Days': ['sum', 'mean'],\n",
          "    'Remaining_Useful_Life_days': ['mean', 'std', 'min', 'max'],\n",
          "    'Health_Score': ['mean', 'std'],\n",
          "    'Device_Age_Years': ['mean', 'std'],\n",
          "    'Usage_Intensity': ['mean', 'std'],\n",
          "    'Temperature_C': ['mean', 'max'],\n",
          "    'Vibration_mms': ['mean', 'max']\n",
          "}).round(3)\n",
          "\n",
          "# Flatten column names\n",
          "device_detailed_stats.columns = ['_'.join(col).strip() for col in device_detailed_stats.columns.values]\n",
          "device_detailed_stats = device_detailed_stats.rename(columns={\n",
          "    'Machine_ID_count': 'Count',\n",
          "    'Failure_Within_7_Days_sum': 'Total_Failures',\n",
          "    'Failure_Within_7_Days_mean': 'Failure_Rate',\n",
          "    'Remaining_Useful_Life_days_mean': 'Avg_Life_Days',\n",
          "    'Remaining_Useful_Life_days_std': 'Std_Life_Days',\n",
          "    'Remaining_Useful_Life_days_min': 'Min_Life_Days',\n",
          "    'Remaining_Useful_Life_days_max': 'Max_Life_Days',\n",
          "    'Health_Score_mean': 'Avg_Health_Score',\n",
          "    'Health_Score_std': 'Std_Health_Score',\n",
          "    'Device_Age_Years_mean': 'Avg_Age_Years',\n",
          "    'Device_Age_Years_std': 'Std_Age_Years',\n",
          "    'Usage_Intensity_mean': 'Avg_Usage_Intensity',\n",
          "    'Usage_Intensity_std': 'Std_Usage_Intensity',\n",
          "    'Temperature_C_mean': 'Avg_Temperature',\n",
          "    'Temperature_C_max': 'Max_Temperature',\n",
          "    'Vibration_mms_mean': 'Avg_Vibration',\n",
          "    'Vibration_mms_max': 'Max_Vibration'\n",
          "})\n",
          "\n",
          "display(device_detailed_stats)"
        ]
      },

      {
        "cell_type": "markdown",
        "metadata": {},
        "source": [
          "### 🎯 Analisi del Rischio per Categoria"
        ]
      },

      {
        "cell_type": "code",
        "execution_count": null,
        "metadata": {},
        "outputs": [],
        "source": [
          "# Identificazione categorie più a rischio\n",
          "print(\"\\n=== CATEGORIE A RISCHIO ===\")\n",
          "risk_analysis = device_detailed_stats[['Failure_Rate', 'Avg_Health_Score', 'Avg_Life_Days']].copy()\n",
          "risk_analysis['Risk_Score'] = (\n",
          "    risk_analysis['Failure_Rate'] * 0.4 +  # 40% peso al tasso di guasto\n",
          "    (1 - risk_analysis['Avg_Health_Score']) * 0.3 +  # 30% peso alla salute (invertito)\n",
          "    (1 - risk_analysis['Avg_Life_Days'] / risk_analysis['Avg_Life_Days'].max()) * 0.3  # 30% peso alla vita rimanente (invertito)\n",
          ")\n",
          "\n",
          "risk_analysis = risk_analysis.sort_values('Risk_Score', ascending=False)\n",
          "risk_analysis['Risk_Level'] = pd.cut(risk_analysis['Risk_Score'], \n",
          "                                    bins=[0, 0.3, 0.6, 1.0], \n",
          "                                    labels=['BASSO', 'MEDIO', 'ALTO'])\n",
          "\n",
          "print(\"Categorie ordinate per livello di rischio:\")\n",
          "for category, row in risk_analysis.iterrows():\n",
          "    print(f\"  {category}: {row['Risk_Level']} (Score: {row['Risk_Score']:.3f})\")\n",
          "    \n",
          "display(risk_analysis)"
        ]
      },

      {
        "cell_type": "markdown",
        "metadata": {},
        "source": [
          "## 🤖 Raccomandazioni per il Machine Learning\n",
          "\n",
          "Analisi automatica delle caratteristiche del dataset per orientare la scelta dei modelli e delle strategie di training."
        ]
      },

      {
        "cell_type": "code",
        "execution_count": null,
        "metadata": {},
        "outputs": [],
        "source": [
          "# Raccomandazioni per il modeling\n",
          "print(\"\\n=== RACCOMANDAZIONI PER IL MODELING ===\")\n",
          "print(\"\\n📊 Basate sull'analisi del preprocessing:\")\n",
          "\n",
          "# Verifica sbilanciamento del dataset\n",
          "failure_rate = df['Failure_Within_7_Days'].mean() * 100\n",
          "minority_class_pct = min(failure_rate, 100 - failure_rate)\n",
          "\n",
          "if minority_class_pct < 10:\n",
          "    print(\"   🔴 CRITICO: Dataset molto sbilanciato\")\n",
          "    print(\"      → Usa SMOTE o Random Oversampling\")\n",
          "    print(\"      → Considera metriche come F1-score, AUC-ROC\")\n",
          "    print(\"      → Evita accuracy come metrica principale\")\n",
          "\n",
          "# Controllo numero di features\n",
          "numerical_features = df.select_dtypes(include=[np.number]).columns.tolist()\n",
          "if 'Failure_Within_7_Days' in numerical_features:\n",
          "    numerical_features.remove('Failure_Within_7_Days')\n",
          "available_features = len(numerical_features)\n",
          "\n",
          "if available_features > 30:\n",
          "    print(\"   📈 ATTENZIONE: Molte features disponibili\")\n",
          "    print(\"      → Considera feature selection (RFE, SelectKBest)\")\n",
          "    print(\"      → Monitora overfitting con cross-validation\")\n",
          "    print(\"      → Usa regolarizzazione (L1/L2)\")"
        ]
      },

      {
        "cell_type": "code",
        "execution_count": null,
        "metadata": {},
        "outputs": [],
        "source": [
          "# Controllo features con bassa varianza\n",
          "from sklearn.feature_selection import VarianceThreshold\n",
          "selector = VarianceThreshold(threshold=0.01)\n",
          "X_temp = df[numerical_features].fillna(df[numerical_features].median())\n",
          "selector.fit(X_temp)\n",
          "low_variance_features = [feat for feat, selected in zip(numerical_features, selector.get_support()) if not selected]\n",
          "\n",
          "if len(low_variance_features) > 0:\n",
          "    print(f\"   ⚠️ {len(low_variance_features)} features con bassa varianza\")\n",
          "    print(\"      → Considera rimozione features poco informative\")\n",
          "    print(f\"      → Features: {', '.join(low_variance_features[:5])}{'...' if len(low_variance_features) > 5 else ''}\")\n",
          "\n",
          "# Analisi categorie ad alto rischio\n",
          "high_risk_categories = len(risk_analysis[risk_analysis['Risk_Level'] == 'ALTO'])\n",
          "if high_risk_categories > 0:\n",
          "    print(f\"   🎯 {high_risk_categories} categorie ad alto rischio identificate\")\n",
          "    print(\"      → Considera strategie di sampling per categoria\")\n",
          "    print(\"      → Valuta modelli ensemble per gestire diversità\")"
        ]
      },

      {
        "cell_type": "code",
        "execution_count": null,
        "metadata": {},
        "outputs": [],
        "source": [
          "# Controllo correlazioni forti\n",
          "correlation_matrix = df[numerical_features].corr().abs()\n",
          "high_corr_pairs = []\n",
          "for i in range(len(correlation_matrix.columns)):\n",
          "    for j in range(i+1, len(correlation_matrix.columns)):\n",
          "        if correlation_matrix.iloc[i, j] > 0.9:\n",
          "            high_corr_pairs.append((correlation_matrix.columns[i], correlation_matrix.columns[j], correlation_matrix.iloc[i, j]))\n",
          "\n",
          "if len(high_corr_pairs) > 0:\n",
          "    print(f\"   🔗 {len(high_corr_pairs)} coppie di features fortemente correlate (>0.9)\")\n",
          "    print(\"      → Considera rimozione di features ridondanti\")\n",
          "    print(\"      → Usa PCA per riduzione dimensionalità\")\n",
          "    for pair in high_corr_pairs[:3]:  # Mostra solo le prime 3\n",
          "        print(f\"        • {pair[0]} ↔ {pair[1]} (r={pair[2]:.3f})\")"
        ]
      },

      {
        "cell_type": "markdown",
        "metadata": {},
        "source": [
          "### 🎯 Raccomandazioni Finali sui Modelli"
        ]
      },
      
      {
        "cell_type": "code",
        "execution_count": null,
        "metadata": {},
        "outputs": [],
        "source": [
          "# Raccomandazioni sui modelli da testare\n",
          "print(\"\\n🤖 MODELLI RACCOMANDATI:\")\n",
          "print(\"   1. Random Forest - Gestisce bene features miste e missing values\")\n",
          "print(\"   2. XGBoost - Eccellente per dati tabulari e sbilanciati\")\n",
          "print(\"   3. Logistic Regression - Baseline interpretabile\")\n",
          "\n",
          "if high_risk_categories > 1:\n",
          "    print(\"   4. Modelli per categoria - Considera modelli separati per ogni categoria ad alto rischio\")\n",
          "\n",
          "print(\"\\n✅ Preprocessing completato e verificato secondo le specifiche del progetto!\")\n",
          "print(\"📋 Prossimi passi:\")\n",
          "print(\"   1. Eseguire model training (03_model_development.ipynb)\")\n",
          "print(\"   2. Validazione e tuning iperparametri\")\n",
          "print(\"   3. Test con dati realistici\")\n",
          "print(\"   4. Deploy nell'interfaccia web\")\n",
          "\n",
          "# Salvataggio delle statistiche per riferimento futuro\n",
          "print(f\"\\n💾 Dataset finale: {len(df)} campioni, {len(df.columns)} features\")\n",
          "print(f\"📊 Tasso di guasto: {failure_rate:.2f}%\")\n",
          "print(f\"🎯 Categorie di dispositivi: {df['Device_Category'].nunique()}\")\n",
          "print(f\"📈 Features numeriche: {available_features}\")"
        ]
      }
    ]
  }
  ]
}