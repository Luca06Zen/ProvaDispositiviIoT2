{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ü§ñ Sviluppo Modelli di Machine Learning\n",
        "## Predizione Guasti Dispositivi IoT Industriali\n",
        "\n",
        "### Obiettivi:\n",
        "- Sviluppare e confrontare diversi algoritmi di machine learning\n",
        "- Ottimizzare iperparametri dei modelli migliori\n",
        "- Selezionare il modello finale per la produzione\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import delle librerie necessarie\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Librerie per machine learning\n",
        "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, StratifiedKFold\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import (\n",
        "    classification_report, confusion_matrix, roc_auc_score, roc_curve,\n",
        "    accuracy_score, precision_score, recall_score, f1_score\n",
        ")\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.feature_selection import SelectKBest, f_classif, RFE\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from imblearn.pipeline import Pipeline as ImbPipeline\n",
        "\n",
        "try:\n",
        "    import xgboost as xgb\n",
        "    XGBOOST_AVAILABLE = True\n",
        "except ImportError:\n",
        "    print(\"‚ö†Ô∏è XGBoost non disponibile. Installare con: pip install xgboost\")\n",
        "    XGBOOST_AVAILABLE = False\n",
        "\n",
        "import joblib\n",
        "import os\n",
        "from datetime import datetime\n",
        "import json\n",
        "\n",
        "# Configurazione visualizzazioni\n",
        "plt.style.use('seaborn-v0_8')\n",
        "sns.set_palette(\"husl\")\n",
        "%matplotlib inline\n",
        "\n",
        "# Configurazione pandas\n",
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.width', None)\n",
        "pd.set_option('display.max_colwidth', None)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìÅ Caricamento Dati Preprocessati"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Caricamento dei dati preprocessati\n",
        "try:\n",
        "    # Tenta di caricare i dati dalla cartella data/processed\n",
        "    train_data = pd.read_csv('data/processed/train_data.csv')\n",
        "    test_data = pd.read_csv('data/processed/test_data.csv')\n",
        "    print(\"‚úÖ Dati caricati da data/processed/\")\n",
        "except FileNotFoundError:\n",
        "    # Se non trova i file, carica i dati puliti e crea train/test split\n",
        "    try:\n",
        "        df = pd.read_csv('data/processed/cleaned_data.csv')\n",
        "        print(\"üìä Caricamento dati puliti e creazione train/test split\")\n",
        "        \n",
        "        # Separazione features e target\n",
        "        X = df.drop('Failure_Within_7_Days', axis=1)\n",
        "        y = df['Failure_Within_7_Days']\n",
        "        \n",
        "        # Train/test split stratificato\n",
        "        X_train, X_test, y_train, y_test = train_test_split(\n",
        "            X, y, test_size=0.2, random_state=42, stratify=y\n",
        "        )\n",
        "        \n",
        "        # Creazione dei dataframe\n",
        "        train_data = pd.concat([X_train, y_train], axis=1)\n",
        "        test_data = pd.concat([X_test, y_test], axis=1)\n",
        "        \n",
        "        # Salvataggio per uso futuro\n",
        "        os.makedirs('data/processed', exist_ok=True)\n",
        "        train_data.to_csv('data/processed/train_data.csv', index=False)\n",
        "        test_data.to_csv('data/processed/test_data.csv', index=False)\n",
        "        \n",
        "    except FileNotFoundError:\n",
        "        print(\"‚ùå Errore: Eseguire prima 02_data_preprocessing.ipynb\")\n",
        "        raise\n",
        "\n",
        "print(f\"üìä Training set: {train_data.shape}\")\n",
        "print(f\"üìä Test set: {test_data.shape}\")\n",
        "print(f\"üéØ Distribuzione target (train): {train_data['Failure_Within_7_Days'].value_counts().to_dict()}\")\n",
        "\n",
        "# Visualizza le prime righe\n",
        "print(\"\\nüìã Prime 5 righe del training set:\")\n",
        "display(train_data.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üîß Preparazione Features e Target"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Separazione features e target\n",
        "X_train = train_data.drop('Failure_Within_7_Days', axis=1)\n",
        "y_train = train_data['Failure_Within_7_Days']\n",
        "X_test = test_data.drop('Failure_Within_7_Days', axis=1)\n",
        "y_test = test_data['Failure_Within_7_Days']\n",
        "\n",
        "print(\"üîß Preparazione features:\")\n",
        "print(f\"   Features disponibili: {X_train.shape[1]}\")\n",
        "print(f\"   Campioni training: {X_train.shape[0]}\")\n",
        "print(f\"   Campioni test: {X_test.shape[0]}\")\n",
        "\n",
        "# Mostra le features disponibili\n",
        "print(f\"\\nüìã Features disponibili ({len(X_train.columns)}):\")\n",
        "for i, col in enumerate(X_train.columns, 1):\n",
        "    print(f\"   {i:2d}. {col}\")\n",
        "\n",
        "# Identificazione features numeriche e categoriche\n",
        "numeric_features = X_train.select_dtypes(include=[np.number]).columns.tolist()\n",
        "categorical_features = X_train.select_dtypes(include=['object']).columns.tolist()\n",
        "\n",
        "print(f\"\\nüî¢ Features numeriche: {len(numeric_features)}\")\n",
        "print(f\"üè∑Ô∏è Features categoriche: {len(categorical_features)}\")\n",
        "\n",
        "if categorical_features:\n",
        "    print(\"\\nüìù Features categoriche:\")\n",
        "    for feature in categorical_features:\n",
        "        unique_vals = X_train[feature].nunique()\n",
        "        print(f\"   - {feature}: {unique_vals} valori unici\")\n",
        "\n",
        "# Controllo distribuzione classi\n",
        "class_distribution = y_train.value_counts(normalize=True) * 100\n",
        "print(f\"\\nüìä Distribuzione classi (Training):\")\n",
        "for class_val, percentage in class_distribution.items():\n",
        "    class_name = \"Guasto\" if class_val == 1 else \"Normale\"\n",
        "    print(f\"   {class_name} (classe {class_val}): {percentage:.2f}%\")\n",
        "\n",
        "# Determina se il dataset √® sbilanciato\n",
        "minority_class_pct = min(class_distribution.values)\n",
        "is_imbalanced = minority_class_pct < 30\n",
        "\n",
        "if is_imbalanced:\n",
        "    print(f\"‚ö†Ô∏è Dataset sbilanciato rilevato (classe minoritaria: {minority_class_pct:.2f}%)\")\n",
        "    print(\"   ‚Üí SMOTE sar√† applicato durante il training\")\n",
        "else:\n",
        "    print(f\"‚úÖ Dataset bilanciato (classe minoritaria: {minority_class_pct:.2f}%)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üéØ Preprocessing Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Preprocessing delle features categoriche se presenti\n",
        "if categorical_features:\n",
        "    print(\"üîÑ Encoding features categoriche...\")\n",
        "    label_encoders = {}\n",
        "    \n",
        "    for feature in categorical_features:\n",
        "        le = LabelEncoder()\n",
        "        # Fit su tutti i dati per evitare problemi con nuove categorie\n",
        "        all_values = pd.concat([X_train[feature], X_test[feature]]).fillna('Unknown')\n",
        "        le.fit(all_values)\n",
        "        \n",
        "        X_train[feature] = le.transform(X_train[feature].fillna('Unknown'))\n",
        "        X_test[feature] = le.transform(X_test[feature].fillna('Unknown'))\n",
        "        \n",
        "        label_encoders[feature] = le\n",
        "        print(f\"   ‚úÖ {feature}: {len(le.classes_)} categorie\")\n",
        "    \n",
        "    # Salvataggio encoders\n",
        "    os.makedirs('data/models', exist_ok=True)\n",
        "    joblib.dump(label_encoders, 'data/models/label_encoders.pkl')\n",
        "    print(\"üíæ Label encoders salvati\")\n",
        "\n",
        "# Gestione valori mancanti nelle features numeriche\n",
        "print(\"\\nüîç Controllo valori mancanti:\")\n",
        "missing_train = X_train.isnull().sum()\n",
        "missing_test = X_test.isnull().sum()\n",
        "\n",
        "if missing_train.sum() > 0 or missing_test.sum() > 0:\n",
        "    print(\"‚ö†Ô∏è Valori mancanti rilevati - applicazione strategia di riempimento\")\n",
        "    # Riempi con la mediana per le features numeriche\n",
        "    for feature in numeric_features:\n",
        "        if X_train[feature].isnull().sum() > 0:\n",
        "            median_val = X_train[feature].median()\n",
        "            X_train[feature].fillna(median_val, inplace=True)\n",
        "            X_test[feature].fillna(median_val, inplace=True)\n",
        "            print(f\"   ‚úÖ {feature}: riempito con mediana ({median_val:.2f})\")\n",
        "else:\n",
        "    print(\"‚úÖ Nessun valore mancante\")\n",
        "\n",
        "# Standardizzazione features numeriche\n",
        "print(\"\\nüìè Standardizzazione features numeriche...\")\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Salvataggio scaler\n",
        "joblib.dump(scaler, 'data/models/scaler.pkl')\n",
        "print(\"üíæ Scaler salvato\")\n",
        "\n",
        "# Conversione in DataFrame per mantenere nomi delle colonne\n",
        "X_train_scaled = pd.DataFrame(X_train_scaled, columns=X_train.columns, index=X_train.index)\n",
        "X_test_scaled = pd.DataFrame(X_test_scaled, columns=X_test.columns, index=X_test.index)\n",
        "\n",
        "print(f\"üìä Shape finale - Train: {X_train_scaled.shape}, Test: {X_test_scaled.shape}\")\n",
        "\n",
        "# Controllo finale\n",
        "print(\"\\nüîç Controllo finale dati:\")\n",
        "print(f\"   Valori infiniti (train): {np.isinf(X_train_scaled.values).sum()}\")\n",
        "print(f\"   Valori NaN (train): {np.isnan(X_train_scaled.values).sum()}\")\n",
        "print(f\"   Range valori (train): [{X_train_scaled.values.min():.3f}, {X_train_scaled.values.max():.3f}]\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìà Feature Selection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Feature Selection con SelectKBest\n",
        "print(\"üéØ Feature Selection...\")\n",
        "\n",
        "# Selezione delle top K features pi√π significative\n",
        "k_best = min(15, X_train_scaled.shape[1])  # Massimo 15 features o tutte se meno di 15\n",
        "selector = SelectKBest(score_func=f_classif, k=k_best)\n",
        "X_train_selected = selector.fit_transform(X_train_scaled, y_train)\n",
        "X_test_selected = selector.transform(X_test_scaled)\n",
        "\n",
        "# Ottieni nomi delle features selezionate\n",
        "selected_features = X_train_scaled.columns[selector.get_support()].tolist()\n",
        "feature_scores = selector.scores_[selector.get_support()]\n",
        "\n",
        "print(f\"‚úÖ Selezionate {len(selected_features)} features su {X_train_scaled.shape[1]}\")\n",
        "\n",
        "# Creazione DataFrame per feature importance\n",
        "feature_importance_df = pd.DataFrame({\n",
        "    'Feature': selected_features,\n",
        "    'Score': feature_scores\n",
        "}).sort_values('Score', ascending=False)\n",
        "\n",
        "print(f\"\\nüèÜ Top {min(10, len(selected_features))} features pi√π importanti:\")\n",
        "for i, (_, row) in enumerate(feature_importance_df.head(10).iterrows()):\n",
        "    print(f\"   {i+1:2d}. {row['Feature']:<30} (Score: {row['Score']:.2f})\")\n",
        "\n",
        "# Visualizzazione feature importance\n",
        "plt.figure(figsize=(12, 6))\n",
        "top_features = feature_importance_df.head(10)\n",
        "plt.barh(range(len(top_features)), top_features['Score'], color='skyblue')\n",
        "plt.yticks(range(len(top_features)), top_features['Feature'])\n",
        "plt.xlabel('Feature Importance Score')\n",
        "plt.title('Top 10 Features pi√π Importanti (SelectKBest F-Score)')\n",
        "plt.gca().invert_yaxis()\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Salvataggio feature selector\n",
        "joblib.dump(selector, 'data/models/feature_selector.pkl')\n",
        "print(\"\\nüíæ Feature selector salvato\")\n",
        "\n",
        "# Salvataggio lista features selezionate\n",
        "with open('data/models/selected_features.json', 'w') as f:\n",
        "    json.dump(selected_features, f, indent=2)\n",
        "print(\"üíæ Lista features selezionate salvata\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ü§ñ Definizione e Training Modelli"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Definizione modelli da testare (secondo le indicazioni del PDF)\n",
        "models = {\n",
        "    'Random Forest': RandomForestClassifier(\n",
        "        n_estimators=100,\n",
        "        max_depth=10,\n",
        "        min_samples_split=5,\n",
        "        min_samples_leaf=2,\n",
        "        random_state=42,\n",
        "        n_jobs=-1\n",
        "    ),\n",
        "    'Decision Tree': DecisionTreeClassifier(\n",
        "        max_depth=10,\n",
        "        min_samples_split=5,\n",
        "        min_samples_leaf=2,\n",
        "        random_state=42\n",
        "    ),\n",
        "    'KNN': KNeighborsClassifier(\n",
        "        n_neighbors=5,\n",
        "        weights='distance'\n",
        "    ),\n",
        "    'SVM': SVC(\n",
        "        kernel='rbf',\n",
        "        probability=True,\n",
        "        random_state=42,\n",
        "        class_weight='balanced'\n",
        "    ),\n",
        "    'Logistic Regression': LogisticRegression(\n",
        "        random_state=42,\n",
        "        max_iter=1000,\n",
        "        class_weight='balanced'\n",
        "    ),\n",
        "    'Gradient Boosting': GradientBoostingClassifier(\n",
        "        n_estimators=100,\n",
        "        learning_rate=0.1,\n",
        "        max_depth=6,\n",
        "        random_state=42\n",
        "    )\n",
        "}\n",
        "\n",
        "# Aggiungi XGBoost se disponibile\n",
        "if XGBOOST_AVAILABLE:\n",
        "    models['XGBoost'] = xgb.XGBClassifier(\n",
        "        n_estimators=100,\n",
        "        max_depth=6,\n",
        "        learning_rate=0.1,\n",
        "        random_state=42,\n",
        "        eval_metric='logloss'\n",
        "    )\n",
        "\n",
        "print(f\"ü§ñ Modelli da testare ({len(models)}): {list(models.keys())}\")\n",
        "\n",
        "# Setup per cross-validation\n",
        "cv_folds = 5\n",
        "cv_strategy = StratifiedKFold(n_splits=cv_folds, shuffle=True, random_state=42)\n",
        "\n",
        "# Storage per risultati\n",
        "model_results = {}\n",
        "trained_models = {}\n",
        "\n",
        "print(f\"\\nüîÑ Configurazione Cross-Validation: {cv_folds}-fold stratificato\")\n",
        "print(f\"üìä Dataset bilanciamento: {'SMOTE applicato' if is_imbalanced else 'Nessun bilanciamento necessario'}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Training e valutazione modelli\n",
        "print(\"üöÄ Inizio training modelli...\\n\")\n",
        "\n",
        "for model_name, model in models.items():\n",
        "    print(f\"üîÑ Training {model_name}...\")\n",
        "    \n",
        "    try:\n",
        "        start_time = datetime.now()\n",
        "        \n",
        "        # Crea pipeline con SMOTE se dataset sbilanciato\n",
        "        if is_imbalanced:\n",
        "            pipeline = ImbPipeline([\n",
        "                ('smote', SMOTE(random_state=42)),\n",
        "                ('classifier', model)\n",
        "            ])\n",
        "        else:\n",
        "            pipeline = model\n",
        "        \n",
        "        # Cross-validation\n",
        "        print(\"   üìä Esecuzione cross-validation...\")\n",
        "        cv_scores = cross_val_score(\n",
        "            pipeline, X_train_selected, y_train, \n",
        "            cv=cv_strategy, scoring='roc_auc', n_jobs=-1\n",
        "        )\n",
        "        \n",
        "        # Training su tutto il training set\n",
        "        print(\"   üèãÔ∏è Training su training set completo...\")\n",
        "        pipeline.fit(X_train_selected, y_train)\n",
        "        \n",
        "        # Predizioni\n",
        "        print(\"   üîÆ Generazione predizioni...\")\n",
        "        y_pred = pipeline.predict(X_test_selected)\n",
        "        y_pred_proba = pipeline.predict_proba(X_test_selected)[:, 1]\n",
        "        \n",
        "        # Calcolo metriche complete\n",
        "        accuracy = accuracy_score(y_test, y_pred)\n",
        "        precision = precision_score(y_test, y_pred)\n",
        "        recall = recall_score(y_test, y_pred)\n",
        "        f1 = f1_score(y_test, y_pred)\n",
        "        roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
        "        \n",
        "        # Tempo di training\n",
        "        training_time = (datetime.now() - start_time).total_seconds()\n",
        "        \n",
        "        # Storage risultati\n",
        "        model_results[model_name] = {\n",
        "            'cv_scores': cv_scores,\n",
        "            'cv_mean': cv_scores.mean(),\n",
        "            'cv_std': cv_scores.std(),\n",
        "            'test_accuracy': accuracy,\n",
        "            'test_precision': precision,\n",
        "            'test_recall': recall,\n",
        "            'test_f1': f1,\n",
        "            'test_roc_auc': roc_auc,\n",
        "            'predictions': y_pred,\n",
        "            'probabilities': y_pred_proba,\n",
        "            'training_time': training_time\n",
        "        }\n",
        "        \n",
        "        trained_models[model_name] = pipeline\n",
        "        \n",
        "        # Output risultati\n",
        "        print(f\"   ‚úÖ CV AUC: {cv_scores.mean():.4f} (¬±{cv_scores.std():.4f})\")\n",
        "        print(f\"   ‚úÖ Test Accuracy: {accuracy:.4f}\")\n",
        "        print(f\"   ‚úÖ Test Precision: {precision:.4f}\")\n",
        "        print(f\"   ‚úÖ Test Recall: {recall:.4f}\")\n",
        "        print(f\"   ‚úÖ Test F1-Score: {f1:.4f}\")\n",
        "        print(f\"   ‚úÖ Test AUC: {roc_auc:.4f}\")\n",
        "        print(f\"   ‚è±Ô∏è Training time: {training_time:.2f}s\")\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"   ‚ùå Errore durante training: {str(e)}\")\n",
        "        continue\n",
        "    \n",
        "    print()\n",
        "\n",
        "print(\"üéâ Training completato per tutti i modelli!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìä Confronto Performance Modelli"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Creazione DataFrame per confronto risultati\n",
        "results_data = []\n",
        "for model_name, results in model_results.items():\n",
        "    results_data.append({\n",
        "        'Model': model_name,\n",
        "        'CV_AUC_Mean': results['cv_mean'],\n",
        "        'CV_AUC_Std': results['cv_std'],\n",
        "        'Test_Accuracy': results['test_accuracy'],\n",
        "        'Test_Precision': results['test_precision'],\n",
        "        'Test_Recall': results['test_recall'],\n",
        "        'Test_F1': results['test_f1'],\n",
        "        'Test_AUC': results['test_roc_auc'],\n",
        "        'Training_Time': results['training_time']\n",
        "    })\n",
        "\n",
        "results_df = pd.DataFrame(results_data)\n",
        "\n",
        "# Ordina per performance AUC su test set\n",
        "results_df = results_df.sort_values('Test_AUC', ascending=False)\n",
        "\n",
        "print(\"üìä RISULTATI CONFRONTO MODELLI:\")\n",
        "print(\"=\" * 100)\n",
        "print(f\"{'Modello':<18} {'CV AUC':<13} {'Accuracy':<9} {'Precision':<10} {'Recall':<8} {'F1-Score':<9} {'Test AUC':<9} {'Time(s)':<8} {'Rank'}\")\n",
        "print(\"=\" * 100)\n",
        "\n",
        "for i, (_, row) in enumerate(results_df.iterrows()):\n",
        "    cv_score = f\"{row['CV_AUC_Mean']:.3f}¬±{row['CV_AUC_Std']:.2f}\"\n",
        "    accuracy = f\"{row['Test_Accuracy']:.4f}\"\n",
        "    precision = f\"{row['Test_Precision']:.4f}\"\n",
        "    recall = f\"{row['Test_Recall']:.4f}\"\n",
        "    f1_score = f\"{row['Test_F1']:.4f}\"\n",
        "    test_auc = f\"{row['Test_AUC']:.4f}\"\n",
        "    time_s = f\"{row['Training_Time']:.1f}\"\n",
        "    rank = f\"#{i+1}\"\n",
        "    \n",
        "    print(f\"{row['Model']:<18} {cv_score:<13} {accuracy:<9} {precision:<10} {recall:<8} {f1_score:<9} {test_auc:<9} {time_s:<8} {rank}\")\n",
        "\n",
        "print(\"=\" * 100)\n",
        "\n",
        "# Identifica il miglior modello\n",
        "best_model_name = results_df.iloc[0]['Model']\n",
        "print(f\"\\nüèÜ MIGLIOR MODELLO: {best_model_name}\")\n",
        "print(f\"   üìä Test AUC: {results_df.iloc[0]['Test_AUC']:.4f}\")\n",
        "print(f\"   üéØ Test F1-Score: {results_df.iloc[0]['Test_F1']:.4f}\")\n",
        "print(f\"   ‚öñÔ∏è Test Precision: {results_df.iloc[0]['Test_Precision']:.4f}\")\n",
        "print(f\"   üîç Test Recall: {results_df.iloc[0]['Test_Recall']:.4f}\")\n",
        "\n",
        "# Salvataggio risultati\n",
        "results_df.to_csv('data/models/model_comparison_results.csv', index=False)\n",
        "print(\"\\nüíæ Risultati salvati in 'data/models/model_comparison_results.csv'\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìà Visualizzazioni Performance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualizzazione confronto metriche principali\n",
        "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
        "fig.suptitle('Confronto Performance Modelli', fontsize=16, fontweight='bold')\n",
        "\n",
        "# 1. Test AUC\n",
        "ax1 = axes[0, 0]\n",
        "bars1 = ax1.bar(results_df['Model'], results_df['Test_AUC'], color='skyblue', alpha=0.8)\n",
        "ax1.set_title('Test AUC Score', fontweight='bold')\n",
        "ax1.set_ylabel('AUC Score')\n",
        "ax1.tick_params(axis='x', rotation=45)\n",
        "ax1.set_ylim(0, 1)\n",
        "# Aggiungi valori sulle barre\n",
        "for bar, value in zip(bars1, results_df['Test_AUC']):\n",
        "    height = bar.get_height()\n",
        "    ax1.text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
        "             f'{value:.3f}', ha='center', va='bottom', fontsize=9)\n",
        "\n",
        "# 2. Test F1-Score\n",
        "ax2 = axes[0, 1]\n",
        "bars2 = ax2.bar(results_df['Model'], results_df['Test_F1'], color='lightcoral', alpha=0.8)\n",
        "ax2.set_title('Test F1-Score', fontweight='bold')\n",
        "ax2.set_ylabel('F1-Score')\n",
        "ax2.tick_params(axis='x', rotation=45)\n",
        "ax2.set_ylim(0, 1)\n",
        "for bar, value in zip(bars2, results_df['Test_F1']):\n",
        "    height = bar.get_height()\n",
        "    ax2.text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
        "             f'{value:.3f}', ha='center', va='bottom', fontsize=9)\n",
        "\n",
        "# 3. Precision vs Recall\n",
        "ax3 = axes[1, 0]\n",
        "x = np.arange(len(results_df))\n",
        "width = 0.35\n",
        "bars3a = ax3.bar(x - width/2, results_df['Test_Precision'], width, label='Precision', color='gold', alpha=0.8)\n",
        "bars3b = ax3.bar(x + width/2, results_df['Test_Recall'], width, label='Recall', color='lightgreen', alpha=0.8)\n",
        "ax3.set_title('Precision vs Recall', fontweight='bold')\n",
        "ax3.set_ylabel('Score')\n",
        "ax3.set_xticks(x)\n",
        "ax3.set_xticklabels(results_df['Model'], rotation=45)\n",
        "ax3.legend()\n",
        "ax3.set_ylim(0, 1)\n",
        "\n",
        "# 4. Training Time\n",
        "ax4 = axes[1, 1]\n",
        "bars4 = ax4.bar(results_df['Model'], results_df['Training_Time'], color='mediumpurple', alpha=0.8)\n",
        "ax4.set_title('Training Time', fontweight='bold')\n",
        "ax4.set_ylabel('Secondi')\n",
        "ax4.tick_params(axis='x', rotation=45)\n",
        "for bar, value in zip(bars4, results_df['Training_Time']):\n",
        "    height = bar.get_height()\n",
        "    ax4.text(bar.get_x() + bar.get_width()/2., height + 0.1,\n",
        "             f'{value:.1f}s', ha='center', va='bottom', fontsize=9)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('data/models/model_performance_comparison.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(\"üìä Grafici salvati in 'data/models/model_performance_comparison.png'\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üéØ Matrice di Confusione e Curve ROC"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualizzazione curve ROC per tutti i modelli\n",
        "plt.figure(figsize=(12, 8))\n",
        "\n",
        "colors = ['blue', 'red', 'green', 'orange', 'purple', 'brown', 'pink']\n",
        "for i, (model_name, results) in enumerate(model_results.items()):\n",
        "    fpr, tpr, _ = roc_curve(y_test, results['probabilities'])\n",
        "    auc_score = results['test_roc_auc']\n",
        "    color = colors[i % len(colors)]\n",
        "    \n",
        "    plt.plot(fpr, tpr, color=color, lw=2, \n",
        "             label=f'{model_name} (AUC = {auc_score:.3f})')\n",
        "\n",
        "# Linea diagonale (classificatore casuale)\n",
        "plt.plot([0, 1], [0, 1], color='black', lw=1, linestyle='--', alpha=0.5)\n",
        "\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('False Positive Rate (1 - Specificit√†)', fontsize=12)\n",
        "plt.ylabel('True Positive Rate (Sensibilit√†)', fontsize=12)\n",
        "plt.title('Curve ROC - Confronto Modelli', fontsize=14, fontweight='bold')\n",
        "plt.legend(loc=\"lower right\", fontsize=10)\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.savefig('data/models/roc_curves_comparison.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(\"üìà Curve ROC salvate in 'data/models/roc_curves_comparison.png'\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Matrice di confusione per i top 3 modelli\n",
        "top_3_models = results_df.head(3)['Model'].tolist()\n",
        "\n",
        "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
        "fig.suptitle('Matrici di Confusione - Top 3 Modelli', fontsize=16, fontweight='bold')\n",
        "\n",
        "for i, model_name in enumerate(top_3_models):\n",
        "    y_pred = model_results[model_name]['predictions']\n",
        "    cm = confusion_matrix(y_test, y_pred)\n",
        "    \n",
        "    # Calcola percentuali\n",
        "    cm_percent = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis] * 100\n",
        "    \n",
        "    ax = axes[i]\n",
        "    sns.heatmap(cm_percent, annot=True, fmt='.1f', cmap='Blues', ax=ax,\n",
        "                xticklabels=['Normale', 'Guasto'],\n",
        "                yticklabels=['Normale', 'Guasto'])\n",
        "    ax.set_title(f'{model_name}\\n(AUC: {model_results[model_name][\"test_roc_auc\"]:.3f})', \n",
        "                 fontweight='bold')\n",
        "    ax.set_xlabel('Predizione')\n",
        "    if i == 0:\n",
        "        ax.set_ylabel('Valore Reale')\n",
        "    \n",
        "    # Aggiungi numeri assoluti in piccolo\n",
        "    for j in range(2):\n",
        "        for k in range(2):\n",
        "            text = ax.texts[j*2 + k]\n",
        "            text.set_text(f'{cm_percent[j,k]:.1f}%\\n({cm[j,k]})')\n",
        "            text.set_fontsize(10)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('data/models/confusion_matrices_top3.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(\"üéØ Matrici di confusione salvate in 'data/models/confusion_matrices_top3.png'\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üîß Ottimizzazione Iperparametri (Miglior Modello)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Ottimizzazione iperparametri per il miglior modello\n",
        "print(f\"üîß Ottimizzazione iperparametri per: {best_model_name}\")\n",
        "\n",
        "# Definizione spazi di ricerca per ogni modello\n",
        "param_grids = {\n",
        "    'Random Forest': {\n",
        "        'classifier__n_estimators': [100, 200, 300],\n",
        "        'classifier__max_depth': [8, 10, 12, None],\n",
        "        'classifier__min_samples_split': [2, 5, 10],\n",
        "        'classifier__min_samples_leaf': [1, 2, 4]\n",
        "    },\n",
        "    'Decision Tree': {\n",
        "        'max_depth': [8, 10, 12, 15, None],\n",
        "        'min_samples_split': [2, 5, 10, 20],\n",
        "        'min_samples_leaf': [1, 2, 5, 10],\n",
        "        'criterion': ['gini', 'entropy']\n",
        "    },\n",
        "    'KNN': {\n",
        "        'n_neighbors': [3, 5, 7, 9, 11],\n",
        "        'weights': ['uniform', 'distance'],\n",
        "        'metric': ['euclidean', 'manhattan']\n",
        "    },\n",
        "    'SVM': {\n",
        "        'C': [0.1, 1, 10, 100],\n",
        "        'gamma': ['scale', 'auto', 0.001, 0.01],\n",
        "        'kernel': ['rbf', 'poly']\n",
        "    },\n",
        "    'Logistic Regression': {\n",
        "        'C': [0.01, 0.1, 1, 10, 100],\n",
        "        'penalty': ['l1', 'l2'],\n",
        "        'solver': ['liblinear', 'saga']\n",
        "    },\n",
        "    'Gradient Boosting': {\n",
        "        'classifier__n_estimators': [100, 200],\n",
        "        'classifier__learning_rate': [0.05, 0.1, 0.2],\n",
        "        'classifier__max_depth': [4, 6, 8],\n",
        "        'classifier__subsample': [0.8, 0.9, 1.0]\n",
        "    }\n",
        "}\n",
        "\n",
        "if XGBOOST_AVAILABLE:\n",
        "    param_grids['XGBoost'] = {\n",
        "        'classifier__n_estimators': [100, 200],\n",
        "        'classifier__learning_rate': [0.05, 0.1, 0.2],\n",
        "        'classifier__max_depth': [4, 6, 8],\n",
        "        'classifier__subsample': [0.8, 0.9]\n",
        "    }\n",
        "\n",
        "# Ottimizzazione del miglior modello\n",
        "if best_model_name in param_grids:\n",
        "    print(f\"üìä Avvio GridSearchCV per {best_model_name}...\")\n",
        "    \n",
        "    best_pipeline = trained_models[best_model_name]\n",
        "    param_grid = param_grids[best_model_name]\n",
        "    \n",
        "    # Aggiusta i nomi dei parametri se necessario per pipeline con SMOTE\n",
        "    if is_imbalanced and best_model_name not in ['KNN', 'SVM', 'Logistic Regression', 'Decision Tree']:\n",
        "        # I parametri sono gi√† prefissati con 'classifier__'\n",
        "        pass\n",
        "    elif is_imbalanced:\n",
        "        # Aggiungi prefisso classifier__ per modelli semplici in pipeline\n",
        "        new_param_grid = {}\n",
        "        for key, value in param_grid.items():\n",
        "            new_param_grid[f'classifier__{key}'] = value\n",
        "        param_grid = new_param_grid\n",
        "    \n",
        "    # Setup GridSearch\n",
        "    grid_search = GridSearchCV(\n",
        "        estimator=best_pipeline,\n",
        "        param_grid=param_grid,\n",
        "        cv=StratifiedKFold(n_splits=3, shuffle=True, random_state=42),  # 3-fold per velocit√†\n",
        "        scoring='roc_auc',\n",
        "        n_jobs=-1,\n",
        "        verbose=1\n",
        "    )\n",
        "    \n",
        "    # Esecuzione ottimizzazione\n",
        "    start_time = datetime.now()\n",
        "    grid_search.fit(X_train_selected, y_train)\n",
        "    optimization_time = (datetime.now() - start_time).total_seconds()\n",
        "    \n",
        "    print(f\"\\n‚úÖ Ottimizzazione completata in {optimization_time:.1f}s\")\n",
        "    print(f\"üèÜ Miglior score CV: {grid_search.best_score_:.4f}\")\n",
        "    print(f\"üéØ Migliori parametri:\")\n",
        "    for param, value in grid_search.best_params_.items():\n",
        "        print(f\"   {param}: {value}\")\n",
        "    \n",
        "    # Valutazione modello ottimizzato su test set\n",
        "    optimized_model = grid_search.best_estimator_\n",
        "    y_pred_opt = optimized_model.predict(X_test_selected)\n",
        "    y_pred_proba_opt = optimized_model.predict_proba(X_test_selected)[:, 1]\n",
        "    \n",
        "    # Metriche modello ottimizzato\n",
        "    opt_accuracy = accuracy_score(y_test, y_pred_opt)\n",
        "    opt_precision = precision_score(y_test, y_pred_opt)\n",
        "    opt_recall = recall_score(y_test, y_pred_opt)\n",
        "    opt_f1 = f1_score(y_test, y_pred_opt)\n",
        "    opt_roc_auc = roc_auc_score(y_test, y_pred_proba_opt)\n",
        "    \n",
        "    print(f\"\\nüìä PERFORMANCE MODELLO OTTIMIZZATO:\")\n",
        "    print(f\"   Accuracy:  {opt_accuracy:.4f} (prima: {model_results[best_model_name]['test_accuracy']:.4f})\")\n",
        "    print(f\"   Precision: {opt_precision:.4f} (prima: {model_results[best_model_name]['test_precision']:.4f})\")\n",
        "    print(f\"   Recall:    {opt_recall:.4f} (prima: {model_results[best_model_name]['test_recall']:.4f})\")\n",
        "    print(f\"   F1-Score:  {opt_f1:.4f} (prima: {model_results[best_model_name]['test_f1']:.4f})\")\n",
        "    print(f\"   AUC:       {opt_roc_auc:.4f} (prima: {model_results[best_model_name]['test_roc_auc']:.4f})\")\n",
        "    \n",
        "    # Salvataggio modello ottimizzato\n",
        "    final_model = optimized_model\n",
        "    final_model_info = {\n",
        "        'model_name': best_model_name,\n",
        "        'is_optimized': True,\n",
        "        'best_params': grid_search.best_params_,\n",
        "        'cv_score': grid_search.best_score_,\n",
        "        'test_metrics': {\n",
        "            'accuracy': opt_accuracy,\n",
        "            'precision': opt_precision,\n",
        "            'recall': opt_recall,\n",
        "            'f1_score': opt_f1,\n",
        "            'roc_auc': opt_roc_auc\n",
        "        }\n",
        "    }\n",
        "    \n",
        "else:\n",
        "    print(f\"‚ö†Ô∏è Griglia parametri non definita per {best_model_name}\")\n",
        "    print(\"üîÑ Utilizzo modello base come finale\")\n",
        "    final_model = trained_models[best_model_name]\n",
        "    final_model_info = {\n",
        "        'model_name': best_model_name,\n",
        "        'is_optimized': False,\n",
        "        'test_metrics': {\n",
        "            'accuracy': model_results[best_model_name]['test_accuracy'],\n",
        "            'precision': model_results[best_model_name]['test_precision'],\n",
        "            'recall': model_results[best_model_name]['test_recall'],\n",
        "            'f1_score': model_results[best_model_name]['test_f1'],\n",
        "            'roc_auc': model_results[best_model_name]['test_roc_auc']\n",
        "        }\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üíæ Salvataggio Modello Finale"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Salvataggio modello finale e informazioni correlate\n",
        "print(\"üíæ Salvataggio modello finale e componenti...\")\n",
        "\n",
        "# Creazione cartella modelli se non esiste\n",
        "os.makedirs('data/models', exist_ok=True)\n",
        "\n",
        "# Salvataggio modello principale\n",
        "joblib.dump(final_model, 'data/models/final_model.pkl')\n",
        "print(\"‚úÖ Modello finale salvato: 'data/models/final_model.pkl'\")\n",
        "\n",
        "# Salvataggio informazioni modello\n",
        "with open('data/models/final_model_info.json', 'w') as f:\n",
        "    # Converti numpy types in tipi Python per JSON\n",
        "    json_info = json.loads(json.dumps(final_model_info, default=str))\n",
        "    json.dump(json_info, f, indent=2)\n",
        "print(\"‚úÖ Info modello salvate: 'data/models/final_model_info.json'\")\n",
        "\n",
        "# Salvataggio tutti i modelli addestrati per confronto futuro\n",
        "for model_name, model in trained_models.items():\n",
        "    safe_name = model_name.replace(' ', '_').lower()\n",
        "    joblib.dump(model, f'data/models/model_{safe_name}.pkl')\n",
        "print(f\"‚úÖ Tutti i modelli salvati ({len(trained_models)} modelli)\")\n",
        "\n",
        "# Salvataggio risultati completi\n",
        "with open('data/models/all_results.json', 'w') as f:\n",
        "    # Prepara i risultati per JSON (rimuovi array numpy)\n",
        "    json_results = {}\n",
        "    for model_name, results in model_results.items():\n",
        "        json_results[model_name] = {\n",
        "            'cv_mean': float(results['cv_mean']),\n",
        "            'cv_std': float(results['cv_std']),\n",
        "            'test_accuracy': float(results['test_accuracy']),\n",
        "            'test_precision': float(results['test_precision']),\n",
        "            'test_recall': float(results['test_recall']),\n",
        "            'test_f1': float(results['test_f1']),\n",
        "            'test_roc_auc': float(results['test_roc_auc']),\n",
        "            'training_time': float(results['training_time'])\n",
        "        }\n",
        "    json.dump(json_results, f, indent=2)\n",
        "print(\"‚úÖ Tutti i risultati salvati: 'data/models/all_results.json'\")\n",
        "\n",
        "print(f\"\\nüéâ MODELLO FINALE PRONTO: {final_model_info['model_name']}\")\n",
        "print(f\"üìä Performance finale (AUC): {final_model_info['test_metrics']['roc_auc']:.4f}\")\n",
        "print(f\"üéØ Performance finale (F1):  {final_model_info['test_metrics']['f1_score']:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üß™ Test con Input Inventati ma Realistici"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Creazione di 3 input di test realistici come richiesto nel PDF\n",
        "print(\"üß™ TEST CON INPUT INVENTATI MA REALISTICI\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Analisi delle statistiche del training set per creare input realistici\n",
        "feature_stats = train_data.describe()\n",
        "\n",
        "# Definizione di 3 scenari di test\n",
        "test_scenarios = [\n",
        "    {\n",
        "        'name': 'üü¢ Dispositivo in Buone Condizioni',\n",
        "        'description': 'Laser_Cutter con parametri ottimali, manutenzione recente',\n",
        "        'Machine_Type': 'Laser_Cutter',\n",
        "        'Installation_Year': 2020,\n",
        "        'Operational_Hours': 2500.0,\n",
        "        'Temperature_C': 22.0,\n",
        "        'Vibration_mms': 1.2,\n",
        "        'Sound_dB': 45.0,\n",
        "        'Oil_Level_pct': 85.0,\n",
        "        'Coolant_Level_pct': 90.0,\n",
        "        'Power_Consumption_kW': 12.5,\n",
        "        'Last_Maintenance_Days_Ago': 15,\n",
        "        'Maintenance_History_Count': 12,\n",
        "        'Failure_History_Count': 1,\n",
        "        'AI_Supervision': 1,\n",
        "        'Error_Codes_Last_30_Days': 0,\n",
        "        'AI_Override_Events': 0,\n",
        "        'Laser_Intensity': 75.0,\n",
        "        'expected_outcome': 'Basso rischio guasto'\n",
        "    },\n",
        "    {\n",
        "        'name': 'üü° Dispositivo con Segnali di Attenzione',\n",
        "        'description': 'Hydraulic_Press con alcuni parametri preoccupanti',\n",
        "        'Machine_Type': 'Hydraulic_Press',\n",
        "        'Installation_Year': 2015,\n",
        "        'Operational_Hours': 8500.0,\n",
        "        'Temperature_C': 35.0,\n",
        "        'Vibration_mms': 4.8,\n",
        "        'Sound_dB': 72.0,\n",
        "        'Oil_Level_pct': 45.0,\